{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Notes\n",
    "\n",
    "These Statistics Notes are designed for a person who is either (1) completely oblivious to the world of statistics or (2) in need of a point of clarification. To cater to (1), it will be in layman's terms with as many tangible and relatable examples as possible. To cater to (2), there is a conscious focus on text explanations as opposed to mathematical formulae (which will still be referenced in these Notes).\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC TERMS AND CONCEPTS: True Population and Sample Population\n",
    "\n",
    "## Introduction \n",
    "\n",
    "2 + 3 = 5. Nobody can dispute that. You are 100% confident that, in all instances, 2 + 3 will yield 5. However, the world has very little of bright-line truths. Which marketing promotion will be the most lucrative? Where will a shipwrecked person be after X days based on the ocean current? How accurate is this MRI scan in showing whether or not a person has disease Y? The world is measured in probabilities, percentages, and predictions - all based on only _some_ available data. \n",
    "\n",
    "To use an example: imagine needing to survey people across the country for who the people are going to vote for. It is unfeasible to ask every single eligible voting person to get the information that you need. In other words, you are working with only _some_ information to get to an answer of who is going to win an election based on voting results for an _entire_ country. The method of (and mathematics behind) extrapolating obtained survey information to provide an answer that does, in fact, reflect the opinions of the entire country, is called __STATISTICS!__\n",
    "\n",
    "There are many definitions of what the field of \"statistics\" is. To offer my own definition (since these are, after all, my notes): __statistics is the *measurement* of probability and confidence that your estimation and results from sample data values accurately reflect the entire population that you are investigating.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "# Difference between Central Limit Theorem and Law of Large Numbers\n",
    "\n",
    "___As sample size approaches total to population...___\n",
    "<br><br>\n",
    "### Central Limit Theorem states...\n",
    "sample distribution will approach normal distribution. (i.e., ___the SHAPE___)\n",
    "<br><br>\n",
    "### Law of Large Numbers\n",
    "states sample central point of tendency (i.e., mean) will approach population mean (i.e., ___the distribution center's MAXIMUM/MINIMUM___)\n",
    "<br><br>\n",
    "Longer explanation: <br><br>\n",
    "http://mathcentral.uregina.ca/QQ/database/QQ.09.99/yam1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Dispersion in a Population...\n",
    "\n",
    "Should I use __Standard Deviation__ or __Standard Error of the Mean?__\n",
    "\n",
    "_Standard deviation_ measures the overall dispersion of a _population_.\n",
    "\n",
    "_Standard Error of the mean_ measures the dispersion _in relation to a characteristics of a sample, such as a mean._ \n",
    "\n",
    "In other words, __you use Standard error of the mean for error bars/confidence intervals, as those things are usually aggregated _to a mean.___\n",
    "\n",
    "(SEM = stan dev/sqrt(num datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy, Precision, Recall, F-Score, R-Squared Score\n",
    "\n",
    "All different things to identify how _well_ a model works, but they all focus on different things:\n",
    "- __Accuracy__ is a measure of how \"right\" something is ((true positives + true negatives) / all datapoints)\n",
    "    - __R-Squared__ values are specific to regression, but means the same thing in a continuous variable context. R-squared is a measure of how much of the variance in a population is explained by the model. (1 - ((explained variance, as identified by sum of regression errors) / (total variance)))\n",
    "    \n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)\n",
    "<br>\n",
    "- TP = \"A hit\"<br>\n",
    "- TN = \"A rejection\"\n",
    "<br>\n",
    "- FP = Type I error (\"False Alarm\") <br>\n",
    "- FN = Type II error (\"A miss\")\n",
    "<br>\n",
    "- __Precision__ is a measure of the accuracy of _all positive values_ (Precision = TP / (TP + FP)). Stated another way, it's a measure of \"how many positives were predicted correctly.\"\n",
    "- __Recall__ (a/k/a \"Sensitivity\") is a measure of the accuracy of _the true positive values_ (Recall = TP / (TP + FN)). Stated another way, it's a measure of \"out of all positive predictions, how many of those were correct?\"\n",
    "    - The opposite of recall is __Specificity__, which measures how many of the negative predictions were correct negative predictions (Specificity = TN / (TN + FP))\n",
    "    - The __F-Score (or \"F1 Score\")__ is a combination of precision and recall: \n",
    "        - F1-Score =  ((recall x precision) / (_B_^2 x recall + precision), where Beta is a coefficient to put weight on either recall or precision - the user's choice)\n",
    "        - A simple __F1 score formula is: 2 x (recall x precision) / (recall + precision)__\n",
    "        - Best F1 score == 1, worst == 0\n",
    "        \n",
    "- Determining the binarization point (for categorical classification problems) that maximizes sensitivity and specificity is via the __ROC curve.__ (See other notes for more explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Confusion_matrix.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization of above:\n",
    "from IPython.display import Image\n",
    "Image(url = 'https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals\n",
    "\n",
    "## What is a Confidence Interval?\n",
    "\n",
    "_You do not expect every single data point to **exactly** reflect every single real data point in the true population. That doesn't mean your statistics is \"wrong\" though - statistics is all about measuring the \"range of certainty\" and the likewise measuring the probability that your statistical value is within a reasonable \"range\" to reflect the actual true population value. **That range is our confidence interval.**_\n",
    "\n",
    "<br>\n",
    "\n",
    "__Short Answer:__ It is the level of certainty that your statistical values that you obtained (the ___sample population___) accurately reflects the ___true population.___\n",
    "\n",
    "__Longer Answer and Example:__ A confidence interval is the range that you believe the specific statistical value is in. For example, if I have a confidence interval of 98%, which gives me a range of values between 4.55 and 6.20, then this means that if the experiment were repeated, there is a 98% chance that the resulting values would be within that range.  Stated more accurately, it demonstrates how \"X% of the time the true population value (the ___parameter___) will fall within CI estimates (the ___sample statistic___(al range, i.e., the sample population)).\"\n",
    "\n",
    "__IMPORTANT POINT!:__ For a _normal distribution_, there is a __95% chance__ that the sample population statistic is within _two standard deviations_ of the population mean! __See__ [the Empirical Rule (a/k/a the 68-95-99.7 (for 1, 2, 3 sigma) Rule)](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule) for normally distributed populations.\n",
    "\n",
    "### Trade-Off Between Accuracy and Precision using Confidence Intervals.\n",
    "\n",
    "A higher confidence interval is the same thing as saying a larger confidence interval range. To use the above referenced Empirical Rule, \"there is a 99.7% chance that values within the sample population accurately reflect values within _3 standard deviations_ of the true population.\"\n",
    "\n",
    "What does that mean? That just mean that you are more confident _because_ your ranges of values is larger. Taking it to the extreme, you can offer a range of 20 standard deviations above and below the mean, and have a confidence interval of 100% that that ENORMOUS range of values would encapsulate and thus reflect the actual values of the true population.\n",
    "\n",
    "As you can intuitively imagine, doing that isn't that helpful. You want to offer some ___precision___ (offering a tighter range) at the cost that maybe some values will be outside of that range (thus, making you slightly less \"confident\" that your offered range is an ___accurate___ reflection of the values of the true population).\n",
    "\n",
    "__Analogy:__ You are tasked with finding a stationary underwater mine (the _parameter_) in the ocean (the _true population_) and closing off the area of the ocean with buoys to warn travelers that a mine is nearby. Instead of taking readings of every inch of the ocean, you take readings of a particular area known to have underwater mines (i.e., the _sample population_). \n",
    "<br>\n",
    "\n",
    "Based on your sample statistic findings, you close off a certain area (the _confidence interval_) of the ocean with the buoys, saying, for example, \"there is a 95% chance the buoy is within the closed off area within the buoy boundaries.\" You can close off an ever larger area (i.e., higher confidence interval), but because of the lack of likelihood that the mine is outside of the 95% interval (i.e., for a normally distributed sample, beyond 2 standard deviations of the mean sample value), creating such a large area yields a diminishing return and may not be actuarially sound to do so (i.e., the _accuracy-precision tradeoff with confidence intervals_).\n",
    "\n",
    "\n",
    "__Note:__ In data science, we use 95% (2 sigma) confidence intervals. In other fields (e.g., particle physics), it may necessitate a higher level of confidence (5 sigma spread), but not in the vast majority of fields, including data science.\n",
    "\n",
    "### Waitttttt a Minute. A Confidence Interval Sounds Identical to a Margin of Error\n",
    "\n",
    "I would agree with you, but they are indeed different. While a confidence interval focuses on your level of certainty that your sample value accurately reflects the parameter (again, the population value), the __Margin of Error__ is a measurement of the _entire_ distribution of the population and thus measures the amount (\"margin\") above and below your confidence interval to account for the spread. See below for a more detailed description of Margin of Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Standard Error vs. Standard Deviation and Relation to Margin of Error\n",
    "\n",
    "Two points to make in this section:\n",
    "1. What is a Standard Error and what is a Standard Deviation? What is the difference between the two?\n",
    "2. How do these terms relate to the term \"Margin of Error\"?\n",
    "\n",
    "## Standard Error and Standard Deviation\n",
    "\n",
    "__Standard Deviation__ is the degree of spread for the distribution of the _true population_.\n",
    "\n",
    "__Standard Error__ is how far off your sample statistical value is from the true popu\n",
    "\n",
    "degree of spread for the distribution of a _sample population_.\n",
    "\n",
    "In case there is any confusion thus far, remind yourself of this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Transformations\n",
    "\n",
    "(Covered more in Unit 2 and Unit 3 Notes)\n",
    "\n",
    "If data is not normally distributed, a lot of models will not yield accurate results, especially regression models (which require linearity and a number of other assumptions, including normality). To force normalization, you can do a __Box-Cox transformation__ via scipy. There are other transformations too, including exponential, root, and logarithmic transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# T-Test and Z-Tests\n",
    "\n",
    "The following computations are done with _one-tailed testing,_ where you are looking in a particular direction (e.g., is the IQ of a sample population _higher_ than the whole population?).\n",
    "\n",
    "## T-Test\n",
    "\n",
    "A t-test (also known as a _Student's t-Test_), is a measurement of hypothesis testing. It measures the distance between the mean of a sample population from the mean of another sample population. This distance is converted into a _t-value._ Referencing the t-value on a \"T-Chart.\" If a t-value is less than a p-value (usually < 0.05), then one can conclude that the sample two populations are statistically significant; you can reject the null hypothesis.\n",
    " - The __null hypothesis__ states: \"We cannot determine the reason why the data is organized in the way that it is: it is hypothesized that the organization is random.\" In finding statistical significance, we can _reject_ that statement and say that there is a _reason_ why the data is organized in the way that it is (making inferences and references to the given correlated features).\n",
    " \n",
    "__T-Value:__ (sample mean - nullhypothesis mean) / (sample population standard deviation / (sqrt(sample size 'n')))\n",
    " \n",
    "---\n",
    "\n",
    "## Z-Test\n",
    "\n",
    "Whereas a t-test focuses on the _means_ of populations, a z-test focuses on the _dispersion_ (deviation) of populations. \n",
    "- For example, if a teacher claims his students have an IQ of 112 based on a random sampling of 30 students, and the population IQ is 100 +/- 15, is his claim supported by statistical significance, or can he not reject the null hypothesis?\n",
    "    - To do this z-test, we obtain a z-value and compare it with a z-score found in a z-table. If the z-test value is higher than the z-table value, than the null hypothesis can be rejected.\n",
    "    - In this particular example, you use a p-value of 5%. That means you are looking at the dispersion of the last 5% in a normal distribution. If the teacher is claiming that his students are smarter than average (50% mark), then you look at the area between the average and the null hypothesis rejection area (the final 5%). In other words you look at the z-table for the 45% range and get a z-score. If the z-test score (defined by (sample avg - average of whole pop) / (standard dev of whole pop) / sqrt(n samples)) is higher than the z-table score, you can reject the null hypothesis.\n",
    "    <br> <br>\n",
    "    - Another example is having averages from two different tests, which have average scores and deviations for each score, and determine which test the student did better on? The higher z-score would answer that question. (sqrt(n) would be 1).\n",
    "    - A \"z-score\" value in itself means how far away the value is from the mean. If the value is 1.6, the datapoint is \"1.6 standard deviations to the right of the mean.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Dealing with Bad Data\n",
    "\n",
    "Data is never clean, perfect, and complete. We have to critically think why the data is missing? Is is an incidental omission or a systemic error in the surveying/data retrieval process? Statisticians define four types of missing data and their underlying missing data mechanisms:\n",
    "\n",
    "MCAR\n",
    "MAR\n",
    "MNAR\n",
    "CI Completely Ignorable\n",
    "\n",
    "This will tell us whether to drop the data or not drop and determine how to impute instead. See below for the methods by which we choose what to do with bad data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## How to Deal with Outliers\n",
    "\n",
    "- __Determine if it's Good Data:__ First, determine if the entry of the data is an actual value (24 instead of 10) or an incidental error (1000 instead of 10). If it's the latter, impute as necessary. If it's the former, then refer to the other methods.\n",
    "    - Likewise, we need to determine if the datapoint is, indeed, an \"outlier\" and not just a different value. __I define an outlier as outside of 3 standard deviations of the mean (for a normal distribution) _OR_ ((1.5 \\* the total interquartile range + the boundary of the IQR)__. Those values are not set in stone and depend on the expected variance in the population.\n",
    "    \n",
    "    \n",
    "- __Windsorization:__ Creating a range and proportionally modifying everything outside of that range (either too low below or too high above that range) to become within that range.\n",
    "\n",
    "\n",
    "- __Transforming the Data:__ If the outliers arise from an exponential trend, then taking the log of that variable may make things more linear, and in turn, bring those outliers closer to the main data area.\n",
    "\n",
    "\n",
    "- __Dropping the Outliers Outright:__ Good to do if the outliers are, in fact, not representative of anything special. You must have good reason to drop the outliers and be prepared to defend your reasons for doing so.\n",
    "    - One example of this is identifying that the outlier data points have unusual properties that are __not__ representative of the main dataset's data. Because therefore it's not a \"part\" of the main data, it can be excluded. Conversely, if these outliers are a natural part of the population that is being studying, do __not__ remove them.\n",
    "        - __If they must be left in:__ Use non-parametric models (e.g., tree-based models) and non-parametric hypothesis tests (e.g., Z-test (a.k.a \"standard score test\")) on the data because they are robust to outliers.\n",
    "    - One method of dropping outliers is ___trimming___, where values outside of a certain range are dropped. This is different from windsorization in that trimming drops data, whereas windsorization scales (i.e., changes) data. The range outside of which you are trimming can be determined by a Z-score, IQR, or similar method.\n",
    "    \n",
    "    \n",
    "- __Perform a Separate Analysis:__ If there are multiple outliers in a given area (ideally a \"second cluster\" of data), then it may be beneficial to separate that data from the main data and perform an analysis on the main data and an analysis on the extreme data.\n",
    "    - A similar process is called ___binning___ or ___discretization___. Here, we place all values into discrete buckets (\"bins\") of either equal ranges (\"width binning\") or of equal number of datapoints (\"frequency binning\"). You then perform analysis on the bins instead of the datapoints themselves. This is great for skewed data.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## How to Deal with Missing Data\n",
    "\n",
    "- __Imputation:__ Imputting data values into the missing data areas. What data to put in depends on the nature of your data.\n",
    "    - _Global Imputation_: replacing all missing values with a certain value (mean? median?) for a given column.\n",
    "        - It is common to use \"Most Frequent\" with sklearn's SimpleImputer for target classification variables!\n",
    "        - Python's `Impyute` library offers a k-NN feature similarity model that imputes based on the entirety of the dataset. It imputes by the distance of related neighbors - sensitive to outliers as a result though.\n",
    "        - Similarly, Impyute's 'Multivariate Imputation Chained Equation (MICE)' model runs through the model multiple times, each updating its imputation (a process called \"multiple imputation\" instead of single substitution \"single imputation\" methods).\n",
    "        \n",
    "        \n",
    "    - _Bucketed Imputation_: replacing missing values with a certain value (mean/median) for a given bucket, such as the cost of X being imputed by the average cost _grouped by year._ Almost always more preferable than Global Imputation.\n",
    "    \n",
    "    \n",
    "- __Interpolation__: Estimating the missing value based on existing data on that datapoint, kind of like making a regression of the missing variable's datapoint based on other features. Very popular!!!!!\n",
    "\n",
    "\n",
    "- __Dropping Data Rows OR Do Nothing & Carry On:__ Good to do if ___and only if:___\n",
    "    - You have plenty of data already.\n",
    "    - You have a good reason to and are explicitly identifying that you are doing so (or you may be called out for \"manipulating data\", which is unethical).\n",
    "        - An example of a good reason is that the data that _is_ in the rows containing the missing data aren't particularly significant or different.\n",
    "        \n",
    "        https://www.theanalysisfactor.com/mar-and-mcar-missing-data/\n",
    "        \n",
    "        https://www.theanalysisfactor.com/causes-of-missing-data/\n",
    "        \n",
    "        https://www.theanalysisfactor.com/missing-data-criteria-for-choosing-an-effective-approach/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
