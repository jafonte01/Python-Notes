{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 - TensorFlow and Keras\n",
    "\n",
    "## 6.6.1 History of Artificial Neural Networks\n",
    "\n",
    "Most of the history is inapplicable to practical skills that these notes focus on. Basically, ANNs were not used until recently when GoogleBrain made it popular again. A key tool used by GoogleBrain was DistBelief, and its popularity rose due to its open source nature.\n",
    "\n",
    "In 2015, DistBelief was changed in a new iteration, becoming __TensorFlow.__ The competitor-tool to TensorFlow was the non-Google __Keras__. Nowadays, howver, Keras and TensorFlow are integrated, allowing Keras to be used natively with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.2 How TensorFlow Works\n",
    "\n",
    "Tensors are array that have ___ranks.___ Ranks represent the number of dimensions of a tensor-array (or simply, a \"tensor\"). To convert a tensor to an actual NumPy array, use `.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main import\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3, 2, 1]                  # rank 1 (single dimensional vector)\n",
    "[[3, 2], [1, 3]]           # rank 2 (two dimensions)\n",
    "[[[1], [2]], [[1], [2]]]   # rank 3\n",
    "2                          # rank 0 (scalar values have no dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "\n",
    "__Thinkful Definition:__ \"Key object that is a place where things can happen in our model.\" Cool.\n",
    "\n",
    "#### Node - The \"Constant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_const = tf.constant(70)\n",
    "print(node_const) # notice how it prints the node itself, not the constant \"70\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Mathematical Operator\" (Add, Multiply, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_3:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_add = tf.add(node_const, node_const)\n",
    "\n",
    "print(node_add) # every time you re-run it, the name changes to Add_n+1\n",
    "# The value (0) doesn't change because the rank of the constant is always 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_place = tf.placeholder(tf.int32)\n",
    "\n",
    "print(node_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Variable\"\n",
    "\n",
    "Has the properties of a placeholder (it literally \"holds the place of a value\") but with additional features, making it have a _variable value_ instead of a placeholder's constant value.\n",
    "\n",
    "Also, you need to manually initialize them (ugh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "q = tf.Variable([0], tf.float32)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions\n",
    "\n",
    "Instead of simply identifying the node, \"sessions\" actually \"runs\" the node and generates an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(node_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilizing all nodes as an example\n",
    "\n",
    "a = tf.placeholder(tf.int32)\n",
    "\n",
    "# Create an operator node that takes our placeholder and a constant node\n",
    "multiply_by_2 = tf.multiply(a, tf.constant(2))\n",
    "\n",
    "# Run the node to return our output\n",
    "sess.run(multiply_by_2, {a : 3}) # this is what you use a placeholder for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6,   8, 162],\n",
       "       [  4,  62,  26]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the beauty of tensors is that it performs higher matrix operations for you!\n",
    "# (something good for image data analysis? (wink hint wink?))\n",
    "sess.run(multiply_by_2, {a : [[3, 4, 81], [2, 31, 13]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.3 - Example TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting variables\n",
    "# Note that our initial value has to match the data type\n",
    "# so 1 would give an error since it's an int...\n",
    "b = tf.Variable([1.], tf.float32)\n",
    "m = tf.Variable([1.], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# Implement a linear model with shorthand for tf.add() by using '+'\n",
    "# and tf.multiply with '*'\n",
    "linear_model = m * x + b\n",
    "\n",
    "# New variables means we have to initialize again\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x:[1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.04001\n"
     ]
    }
   ],
   "source": [
    "# getting fancier...\n",
    "# creating a loss function and thereby determining accuracy\n",
    "\n",
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]}))\n",
    "\n",
    "# That's a high sum squared loss! Large error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9960036e-16\n"
     ]
    }
   ],
   "source": [
    "# You can go back and adjust the model with the node\n",
    "#########TF.ASSIGN!\n",
    "\n",
    "fixm = tf.assign(m, [-1.])\n",
    "fixb = tf.assign(b, [1.1])\n",
    "sess.run([fixm, fixb])\n",
    "print(sess.run(loss, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^The error is basically zero here, which is great!<br>\n",
    "But how did we know to re-assign m and b to be -1 and 1.1, respectively? Thinkful cheated...\n",
    "\n",
    "What we _can_ do is to create a gradient descent function to minimize the loss function. We can set the gradient descent to be a tensorflow node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[array([-0.9286998], dtype=float32), array([0.89036876], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Set your learning rate in Gradient Descent - 0.01 is just fine\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# reset values to incorrect defaults.\n",
    "# Otherwise, the \"session\" would work off of itself - VERY IMPORTANT!*******\n",
    "sess.run(init) \n",
    "\n",
    "# Loop for 100 iterations, trying to find optimal values\n",
    "for i in range(100):\n",
    "    sess.run(train, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]})\n",
    "\n",
    "print(sess.run([m, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.4 Keras Introduction\n",
    "\n",
    "Keras is more accessible but can do less than TensorFlow (according to Thinkful, of course). Instead of TensorFlow's \"nodes\" and \"variables\", Keras has:\n",
    "\n",
    "__Layers:__ Like layers in a typical neural network model (a set of perceptron nodes).\n",
    "- A layer is called a \"dense\" layer if every node connects to every node in the next layer.\n",
    "\n",
    "__Models:__ The structure of your layering. You \"make a model\" by stacking the layers (see example below). Two flavors of model:\n",
    "- _Sequential_: Stacks of layers in a linear progression.\n",
    "- _Complex_: Non-sequential layering. Obviously such a model is more...ummm...'complex'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "\n",
    "model = Sequential()\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=100, input_dim=100)) # \"adding\" on top of one another \n",
    "model.add(Activation('relu'))              # forms a sequential model\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...I did it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
