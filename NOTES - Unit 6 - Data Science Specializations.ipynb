{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 - TensorFlow and Keras\n",
    "\n",
    "## 6.6.1 History of Artificial Neural Networks\n",
    "\n",
    "Most of the history is inapplicable to practical skills that these notes focus on. Basically, ANNs were not used until recently when GoogleBrain made it popular again. A key tool used by GoogleBrain was DistBelief, and its popularity rose due to its open source nature.\n",
    "\n",
    "In 2015, DistBelief was changed in a new iteration, becoming __TensorFlow.__ The competitor-tool to TensorFlow was the non-Google __Keras__. Nowadays, howver, Keras and TensorFlow are integrated, allowing Keras to be used natively with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.2 How TensorFlow Works\n",
    "\n",
    "Tensors are array that have ___ranks.___ Ranks represent the number of dimensions of a tensor-array (or simply, a \"tensor\"). To convert a tensor to an actual NumPy array, use `.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main import\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3, 2, 1]                  # rank 1 (single dimensional vector)\n",
    "[[3, 2], [1, 3]]           # rank 2 (two dimensions)\n",
    "[[[1], [2]], [[1], [2]]]   # rank 3\n",
    "2                          # rank 0 (scalar values have no dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "\n",
    "__Thinkful Definition:__ \"Key object that is a place where things can happen in our model.\" Cool.\n",
    "\n",
    "#### Node - The \"Constant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_const = tf.constant(70)\n",
    "print(node_const) # notice how it prints the node itself, not the constant \"70\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Mathematical Operator\" (Add, Multiply, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_3:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_add = tf.add(node_const, node_const)\n",
    "\n",
    "print(node_add) # every time you re-run it, the name changes to Add_n+1\n",
    "# The value (0) doesn't change because the rank of the constant is always 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_place = tf.placeholder(tf.int32)\n",
    "\n",
    "print(node_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Variable\"\n",
    "\n",
    "Has the properties of a placeholder (it literally \"holds the place of a value\") but with additional features, making it have a _variable value_ instead of a placeholder's constant value.\n",
    "\n",
    "Also, you need to manually initialize them (ugh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "q = tf.Variable([0], tf.float32)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions\n",
    "\n",
    "Instead of simply identifying the node, \"sessions\" actually \"runs\" the node and generates an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(node_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilizing all nodes as an example\n",
    "\n",
    "a = tf.placeholder(tf.int32)\n",
    "\n",
    "# Create an operator node that takes our placeholder and a constant node\n",
    "multiply_by_2 = tf.multiply(a, tf.constant(2))\n",
    "\n",
    "# Run the node to return our output\n",
    "sess.run(multiply_by_2, {a : 3}) # this is what you use a placeholder for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6,   8, 162],\n",
       "       [  4,  62,  26]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the beauty of tensors is that it performs higher matrix operations for you!\n",
    "# (something good for image data analysis? (wink hint wink?))\n",
    "sess.run(multiply_by_2, {a : [[3, 4, 81], [2, 31, 13]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.3 - Example TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting variables\n",
    "# Note that our initial value has to match the data type\n",
    "# so 1 would give an error since it's an int...\n",
    "b = tf.Variable([1.], tf.float32)\n",
    "m = tf.Variable([1.], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# Implement a linear model with shorthand for tf.add() by using '+'\n",
    "# and tf.multiply with '*'\n",
    "linear_model = m * x + b\n",
    "\n",
    "# New variables means we have to initialize again\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x:[1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.04001\n"
     ]
    }
   ],
   "source": [
    "# getting fancier...\n",
    "# creating a loss function and thereby determining accuracy\n",
    "\n",
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]}))\n",
    "\n",
    "# That's a high sum squared loss! Large error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9960036e-16\n"
     ]
    }
   ],
   "source": [
    "# You can go back and adjust the model with the node\n",
    "#########TF.ASSIGN!\n",
    "\n",
    "fixm = tf.assign(m, [-1.])\n",
    "fixb = tf.assign(b, [1.1])\n",
    "sess.run([fixm, fixb])\n",
    "print(sess.run(loss, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^The error is basically zero here, which is great!<br>\n",
    "But how did we know to re-assign m and b to be -1 and 1.1, respectively? Thinkful cheated...\n",
    "\n",
    "What we _can_ do is to create a gradient descent function to minimize the loss function. We can set the gradient descent to be a tensorflow node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[array([-0.9286998], dtype=float32), array([0.89036876], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Set your learning rate in Gradient Descent - 0.01 is just fine\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# reset values to incorrect defaults.\n",
    "# Otherwise, the \"session\" would work off of itself - VERY IMPORTANT!*******\n",
    "sess.run(init) \n",
    "\n",
    "# Loop for 100 iterations, trying to find optimal values\n",
    "for i in range(100):\n",
    "    sess.run(train, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]})\n",
    "\n",
    "print(sess.run([m, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.4 Keras Introduction\n",
    "\n",
    "Keras is more accessible but can do less than TensorFlow (according to Thinkful, of course). Instead of TensorFlow's \"nodes\" and \"variables\", Keras has:\n",
    "\n",
    "__Layers:__ Like layers in a typical neural network model (a set of perceptron nodes).\n",
    "- A layer is called a \"dense\" layer if every node connects to every node in the next layer.\n",
    "\n",
    "__Models:__ The structure of your layering. You \"make a model\" by stacking the layers (see example below). Two flavors of model:\n",
    "- _Sequential_: Stacks of layers in a linear progression.\n",
    "- _Complex_: Non-sequential layering. Obviously such a model is more...ummm...'complex'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "\n",
    "model = Sequential()\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=100, input_dim=100)) # \"adding\" on top of one another \n",
    "model.add(Activation('relu'))              # forms a sequential model\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# ...I did it. Models need to be compiled and fit later\n",
    "# This will be demonstrated in the next section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.5 Keras MNIST Guided Example \n",
    "\n",
    "__Goal:__ Using the MNIST dataset, we will use neural networks to classify handwritten numbers as the proper digits, 0-9. Note that this data is sparse!!!!!\n",
    "\n",
    "MNIST is a good example of neural network usage. Multiple layers full of values allows for good unsupervised analysis. __To use neural networks, it needs to be fed A LOT of data__ (albeit, a lot of sparse data :( ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "from keras.datasets import mnist # easy to load!!!!!!!!\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() # split and load data\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(28*28)        # MNIST picture resolution\n",
    "print(len(x_train)) # array size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# need to compress/shape the data into flat vectors for each digit\n",
    "\n",
    "# Change shape \n",
    "# Need to reshape 60,000 arrays to length 784, one for each image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, \n",
    "# create 10 binary columns (one for each number!!!!)\n",
    "from keras.utils.np_utils import to_categorical \n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiating our Keras model\n",
    "'''\n",
    "Keras model will use \"dense layers\" and \"dropouts\"\n",
    "\n",
    "***Dense layers*** - layers that are fully connected\n",
    "***Dropout Drops*** - drops out a fraction of the perceptrons to prevent overfitting\n",
    "'''\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "'''\n",
    " Add dense layers to create a fully connected MLP\n",
    " Note that we specify an input shape for the first layer, but\n",
    "***ONLY*** the first layer!!!!!!!!!\n",
    "'''\n",
    "\n",
    "# Relu = \"Rectified Linear Unit (standard activation function)\"\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.4167 - acc: 0.8776 - val_loss: 0.2037 - val_acc: 0.9397\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2020 - acc: 0.9394 - val_loss: 0.1379 - val_acc: 0.9586\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1558 - acc: 0.9541 - val_loss: 0.1200 - val_acc: 0.9639\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1325 - acc: 0.9603 - val_loss: 0.1057 - val_acc: 0.9681\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1149 - acc: 0.9655 - val_loss: 0.1021 - val_acc: 0.9686\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1051 - acc: 0.9684 - val_loss: 0.0933 - val_acc: 0.9709\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0963 - acc: 0.9706 - val_loss: 0.0894 - val_acc: 0.9742\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0894 - acc: 0.9728 - val_loss: 0.0884 - val_acc: 0.9731\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0846 - acc: 0.9742 - val_loss: 0.0879 - val_acc: 0.9747\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0788 - acc: 0.9758 - val_loss: 0.0878 - val_acc: 0.9749\n",
      "Test loss: 0.08778246227339842\n",
      "Test accuracy: 0.9749\n"
     ]
    }
   ],
   "source": [
    "# fitting the instantiated model\n",
    "'''\n",
    "BATCH_SIZE = number of samples to use in each step \n",
    "----larger size = faster (bigger steps) but \n",
    "----decreases accuracy (learning rate not as small step-wise)\n",
    "\n",
    "----Notice how the layers added above add up to 128 (batch size?)\n",
    "----layers are size=64 (see above) 2^x size layers good for parallized computation\n",
    "\n",
    "EPOCH = essentially an iteration of the model - \n",
    "-----each epoch improves on what was learned in the previous iteration/epoch\n",
    "'''\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,     # essentially iterations of the model going through the data\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Convolutional Neural Networks\n",
    "\n",
    "__Convolution:__ Analyzing data via overlapping segments of a given feature upon which it develops its model.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1200/1*GcI7G-JLAQiEoCON7xFbhg.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://cdn-images-1.medium.com/max/1200/1*GcI7G-JLAQiEoCON7xFbhg.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEPS TO CONVOLUTION__\n",
    "1. Define shape of input data (most often, first chunk of code is reshaping the loaded data)\n",
    "2. Create the tiles (or ___kernels___)\n",
    "3. Reduce the sample size into a pooling layer (a process called ___downsampling___)\n",
    "4. Flatten the downsampled data, place flattened data into dense layers and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.2618 - acc: 0.9198 - val_loss: 0.0569 - val_acc: 0.9818\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.0875 - acc: 0.9741 - val_loss: 0.0426 - val_acc: 0.9865\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.0650 - acc: 0.9812 - val_loss: 0.0340 - val_acc: 0.9885\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.0541 - acc: 0.9843 - val_loss: 0.0304 - val_acc: 0.9894\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 132s 2ms/step - loss: 0.0475 - acc: 0.9854 - val_loss: 0.0305 - val_acc: 0.9904\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.0409 - acc: 0.9877 - val_loss: 0.0284 - val_acc: 0.9905\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 131s 2ms/step - loss: 0.0378 - acc: 0.9885 - val_loss: 0.0307 - val_acc: 0.9889\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.0340 - acc: 0.9896 - val_loss: 0.0273 - val_acc: 0.9911\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 0.0336 - acc: 0.9899 - val_loss: 0.0278 - val_acc: 0.9908\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.0296 - acc: 0.9908 - val_loss: 0.0284 - val_acc: 0.9905\n",
      "Test loss: 0.028370814311789217\n",
      "Test accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "# 1. LOAD DATA, SHUFFLE, AND RESHAPE\n",
    "\n",
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# K is \"backend\" work...don't ask\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#---------------------------------------------------\n",
    "# 2. MODEL INSTANTIATION\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 3. MODEL FIT & RUN\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# convolutional NNs take A LONG LONG TIME to run, so be patient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the convolutional model takes much longer...but it yielded a higher accuracy than the previous model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Recurrent Neural Networks\n",
    "\n",
    "Recurrent NNs are different from the above NNs, which are all __feed-forward__ - that is, data flows in one direction until it reaches the end.\n",
    "\n",
    "Recurrent NNs instead \"cycle\" through the network. Accordingly, \"Sequential models\" are no longer an option. Also, this is more complex than feed-forward, meaning it will take even longer (yay...)!\n",
    "\n",
    "Example of a Recurrent Neural Network is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 1.0018 - acc: 0.6530 - val_loss: 0.6586 - val_acc: 0.7763\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.4721 - acc: 0.8445 - val_loss: 0.3947 - val_acc: 0.8691\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.3060 - acc: 0.9035 - val_loss: 0.2397 - val_acc: 0.9229\n",
      "Test loss: 0.23966183296740054\n",
      "Test accuracy: 0.9229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters.\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3      # notice this is much lower than the 10 - \n",
    "                # don't wanna take forever!\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "#------------------------------------------------------------\n",
    "\n",
    "# The data, shuffled, and split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW\n",
    "\n",
    "[This is a good example of how to do a CNN Keras model](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6).\n",
    "\n",
    "__Steps to do a Keras Model:__\n",
    "1. Do basic imports (including various Keras imports) and import data.\n",
    "\n",
    "\n",
    "2. Split and clean the data.\n",
    "\n",
    "\n",
    "3. Normalize the data _values_ (i.e., change pixel values to \\[0,1\\] as neural network converges faster that way).\n",
    "    - In the MNIST dataset, pixel values range from 0 to 255, depending on the pixel's darkness level. To normalize this data, simply divide the features columnwise: \n",
    "        - `X_train = [['pixel_1', 'pixel_2, ... pixel_784]]`\n",
    "        - `X_train = X_train / 255`\n",
    "\n",
    "\n",
    "4. Reshape the data (flatten it to make it an input that's appropriate for a Keras model)\n",
    "    - `X_train = X_train.values.reshape(-1, 28, 28, 1)` representing 28x28 pixels and 1 for 'canal' dimension\n",
    "    - For different neural networks, reshaping syntax may need to be altered (see above examples)\n",
    "    - If target variable is not encoded into numbers, make sure to do that\n",
    "    - Not only do you have to one-hot-encode the values, but you __ALSO__ have to create/dummy columns representing each different categorical value (as opposed to having a one-dimensional array of all the values!!!!) \n",
    "        - You can do the above via `from keras.utils.np_utils import to_categorical` and later `Y_train = to_categorical(Y_train, num_classes = 10)`\n",
    "\n",
    "\n",
    "5. Setting up the Keras model architecture:\n",
    "    - adding neural network layers:\n",
    "        - `model = Sequential()`\n",
    "        - Add as many layers as you want: `model.add(`_(type of NN layer)_`)\n",
    "        - After adding desired layers, do `model.add(Flatten())` and then `model.add(Dense(...))` and `model.add(Dropout())`\n",
    "    - Modify model for efficiency:\n",
    "        - Add optimizer (adjusts model's learning rate)\n",
    "            - `optimizer = RMSprop(...)`\n",
    "            - `model.compile(optimizer=optimizer, loss= ..., metrics=['accuracy']`\n",
    "        - Add annealer (speeds up convergence by adjusting the learning rate if the model is learning too slow or too fast)\n",
    "            - `learningratereduction = ReduceLROnPlateau(...)`\n",
    "        \n",
    "        \n",
    "6. Set up Data augmentation (if the model runs into an overfitting problem, otherwise, ignore)\n",
    "\n",
    "\n",
    "7. Fit the model (and adjust parameters, including the annealer in \"callbacks=\" parameter if necessary)\n",
    "\n",
    "\n",
    "8. Test model's accuracy.\n",
    "\n",
    "\n",
    "9. If using a supervised dataset, use a confusion matrix for individual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible errors:\n",
    "\n",
    "1. Error when calling \"cross_entropy\" \n",
    "    - __Answer:__ shaping error when analyzing the target variable. You need to do y_train (or test) = `keras.utils.to_categorical(y_train, num_classes= (num of categories in target variable))\n",
    "    \n",
    "    \n",
    "2. Array of size __X__ cannot be reshaped using (your dimensions)\n",
    "    - __Answer:__ Your dimensions do not equal the dimensions of the original dataset. If you are having trouble figuring out what dimensions will work, reshape to the original dimensions of the dataset (yes, it is still a \"reshaping\" in the eyes of Keras)\n",
    "\n",
    "\n",
    "3. Layer dense_21 expected (1,) but got (10,)\n",
    "    - __Answer:__ As shown in the above example, the final layer must be the same dimensionality as that of the target variable. If appropriately one-hot encoded (see Possible Error 1 above), then the number of categories encoded to will be the number found in the final model.add(Dense(# here)) layer when creating the Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
