{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Notes\n",
    "\n",
    "These Statistics Notes are designed for a person who is either (1) completely oblivious to the world of statistics or (2) in need of a point of clarification. To cater to (1), it will be in layman's terms with as many tangible and relatable examples as possible. To cater to (2), there is a conscious focus on text explanations as opposed to mathematical formulae (which will still be referenced in these Notes).\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC TERMS AND CONCEPTS: True Population and Sample Population\n",
    "\n",
    "## Introduction \n",
    "\n",
    "2 + 3 = 5. Nobody can dispute that. You are 100% confident that, in all instances, 2 + 3 will yield 5. However, the world has very little of bright-line truths. Which marketing promotion will be the most lucrative? Where will a shipwrecked person be after X days based on the ocean current? How accurate is this MRI scan in showing whether or not a person has disease Y? The world is measured in probabilities, percentages, and predictions - all based on only _some_ available data. \n",
    "\n",
    "To use an example: imagine needing to survey people across the country for who the people are going to vote for. It is unfeasible to ask every single eligible voting person to get the information that you need. In other words, you are working with only _some_ information to get to an answer of who is going to win an election based on voting results for an _entire_ country. The method of (and mathematics behind) extrapolating obtained survey information to provide an answer that does, in fact, reflect the opinions of the entire country, is called __STATISTICS!__\n",
    "\n",
    "There are many definitions of what the field of \"statistics\" is. To offer my own definition (since these are, after all, my notes): __statistics is the *measurement* of probability and confidence that your estimation and results from sample data values accurately reflect the entire population that you are investigating.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Dispersion in a Population...\n",
    "\n",
    "Should I use __Standard Deviation__ or __Standard Error of the Mean?__\n",
    "\n",
    "_Standard deviation_ measures the overall dispersion of a _population_.\n",
    "\n",
    "_Standard Error of the mean_ measures the dispersion _in relation to a characteristics of a sample, such as a mean._ \n",
    "\n",
    "In other words, __you use Standard error of the mean for error bars/confidence intervals, as those things are usually aggregated _to a mean.___\n",
    "\n",
    "(SEM = stan dev/sqrt(num datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy, Precision, Recall, F-Score, R-Squared Score\n",
    "\n",
    "All different things to identify how _well_ a model works, but they all focus on different things:\n",
    "- __Accuracy__ is a measure of how \"right\" something is ((true positives + true negatives) / all datapoints)\n",
    "    - __R-Squared__ values are specific to regression, but means the same thing in a continuous variable context. R-squared is a measure of how much of the variance in a population is explained by the model. (1 - ((explained variance, as identified by sum of regression errors) / (total variance)))\n",
    "    \n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)\n",
    "<br>\n",
    "TP = \"A hit\"<br>\n",
    "TN = \"A rejection\"\n",
    "<br>\n",
    "FP = Type I error (\"False Alarm\") <br>\n",
    "FN = Type II error (\"A miss\")\n",
    "<br>\n",
    "- __Precision__ is a measure of the accuracy of _all positive values_ (Precision = TP / (TP + FP)). Stated another way, it's a measure of \"how many positives were predicted correctly.\"\n",
    "- __Recall__ (a/k/a \"Sensitivity\") is a measure of the accuracy of _the true positive values_ (Recall = TP / (TP + FN)). Stated another way, it's a measure of \"out of all positive predictions, how many of those were correct?\"\n",
    "    - The opposite of recall is __Specificity__, which measures how many of the negative predictions were correct negative predictions (Specificity = TN / (TN + FP))\n",
    "    - The __F-Score (or \"F1 Score\")__ is a combination of precision and recall: ((recall x precision) / (B^2 x recall + precision), where Beta is a coefficient to put weight on either recall or precision - the user's choice)\n",
    "        - A simple F1 score formula is: 2 x (recall x precision) / (recall + precision)\n",
    "        - Best F1 score == 1, worst == 0\n",
    "        \n",
    "- Determining the binarization point (for categorical classification problems) that maximizes sensitivity and specificity is via the __ROC curve.__ (See other notes for more explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Confusion_matrix.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization of above:\n",
    "from IPython.display import Image\n",
    "Image(url = 'https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals\n",
    "\n",
    "## What is a Confidence Interval?\n",
    "\n",
    "_You do not expect every single data point to **exactly** reflect every single real data point in the true population. That doesn't mean your statistics is \"wrong\" though - statistics is all about measuring the \"range of certainty\" and the likewise measuring the probability that your statistical value is within a reasonable \"range\" to reflect the actual true population value. **That range is our confidence interval.**_\n",
    "\n",
    "<br>\n",
    "\n",
    "__Short Answer:__ It is the level of certainty that your statistical values that you obtained (the ___sample population___) accurately reflects the ___true population.___\n",
    "\n",
    "__Longer Answer and Example:__ A confidence interval is the range that you believe the specific statistical value is in. For example, if I have a confidence interval of 98%, which gives me a range of values between 4.55 and 6.20, then this means that if the experiment were repeated, there is a 98% chance that the resulting values would be within that range.  Stated more accurately, it demonstrates how \"X% of the time the true population value (the ___parameter___) will fall within CI estimates (the ___sample statistic___(al range, i.e., the sample population)).\"\n",
    "\n",
    "__IMPORTANT POINT!:__ For a _normal distribution_, there is a __95% chance__ that the sample population statistic is within _two standard deviations_ of the population mean! __See__ [the Empirical Rule (a/k/a the 68-95-99.7 (for 1, 2, 3 sigma) Rule)](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule) for normally distributed populations.\n",
    "\n",
    "### Trade-Off Between Accuracy and Precision using Confidence Intervals.\n",
    "\n",
    "A higher confidence interval is the same thing as saying a larger confidence interval range. To use the above referenced Empirical Rule, \"there is a 99.7% chance that values within the sample population accurately reflect values within _3 standard deviations_ of the true population.\"\n",
    "\n",
    "What does that mean? That just mean that you are more confident _because_ your ranges of values is larger. Taking it to the extreme, you can offer a range of 20 standard deviations above and below the mean, and have a confidence interval of 100% that that ENORMOUS range of values would encapsulate and thus reflect the actual values of the true population.\n",
    "\n",
    "As you can intuitively imagine, doing that isn't that helpful. You want to offer some ___precision___ (offering a tighter range) at the cost that maybe some values will be outside of that range (thus, making you slightly less \"confident\" that your offered range is an ___accurate___ reflection of the values of the true population).\n",
    "\n",
    "__Analogy:__ You are tasked with finding a stationary underwater mine (the _parameter_) in the ocean (the _true population_) and closing off the area of the ocean with buoys to warn travelers that a mine is nearby. Instead of taking readings of every inch of the ocean, you take readings of a particular area known to have underwater mines (i.e., the _sample population_). \n",
    "<br>\n",
    "\n",
    "Based on your sample statistic findings, you close off a certain area (the _confidence interval_) of the ocean with the buoys, saying, for example, \"there is a 95% chance the buoy is within the closed off area within the buoy boundaries.\" You can close off an ever larger area (i.e., higher confidence interval), but because of the lack of likelihood that the mine is outside of the 95% interval (i.e., for a normally distributed sample, beyond 2 standard deviations of the mean sample value), creating such a large area yields a diminishing return and may not be actuarially sound to do so (i.e., the _accuracy-precision tradeoff with confidence intervals_).\n",
    "\n",
    "\n",
    "__Note:__ In data science, we use 95% (2 sigma) confidence intervals. In other fields (e.g., particle physics), it may necessitate a higher level of confidence (5 sigma spread), but not in the vast majority of fields, including data science.\n",
    "\n",
    "### Waitttttt a Minute. A Confidence Interval Sounds Identical to a Margin of Error\n",
    "\n",
    "I would agree with you, but they are indeed different. While a confidence interval focuses on your level of certainty that your sample value accurately reflects the parameter (again, the population value), the __Margin of Error__ is a measurement of the _entire_ distribution of the population and thus measures the amount (\"margin\") above and below your confidence interval to account for the spread. See below for a more detailed description of Margin of Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Standard Error vs. Standard Deviation and Relation to Margin of Error\n",
    "\n",
    "Two points to make in this section:\n",
    "1. What is a Standard Error and what is a Standard Deviation? What is the difference between the two?\n",
    "2. How do these terms relate to the term \"Margin of Error\"?\n",
    "\n",
    "## Standard Error and Standard Deviation\n",
    "\n",
    "__Standard Deviation__ is the degree of spread for the distribution of the _true population_.\n",
    "\n",
    "__Standard Error__ is how far off your sample statistical value is from the true popu\n",
    "\n",
    "degree of spread for the distribution of a _sample population_.\n",
    "\n",
    "In case there is any confusion thus far, remind yourself of this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Transformations\n",
    "\n",
    "(Covered more in Unit 2 and Unit 3 Notes)\n",
    "\n",
    "If data is not normally distributed, a lot of models will not yield accurate results, especially regression models (which require linearity and a number of other assumptions, including normality). To force normalization, you can do a __Box-Cox transformation__ via scipy. There are other transformations too, including exponential, root, and logarithmic transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# T-Test and Z-Tests\n",
    "\n",
    "The following computations are done with _one-tailed testing,_ where you are looking in a particular direction (e.g., is the IQ of a sample population _higher_ than the whole population?).\n",
    "\n",
    "## T-Test\n",
    "\n",
    "A t-test (also known as a _Student's t-Test_), is a measurement of hypothesis testing. It measures the distance between the mean of a sample population from the mean of another sample population. This distance is converted into a _t-value._ Referencing the t-value on a \"T-Chart.\" If a t-value is less than a p-value (usually < 0.05), then one can conclude that the sample two populations are statistically significant; you can reject the null hypothesis.\n",
    " - The __null hypothesis__ states: \"We cannot determine the reason why the data is organized in the way that it is: it is hypothesized that the organization is random.\" In finding statistical significance, we can _reject_ that statement and say that there is a _reason_ why the data is organized in the way that it is (making inferences and references to the given correlated features).\n",
    " \n",
    "__T-Value:__ (sample mean - nullhypothesis mean) / (sample population standard deviation / (sqrt(sample size 'n')))\n",
    " \n",
    "---\n",
    "\n",
    "## Z-Test\n",
    "\n",
    "Whereas a t-test focuses on the _means_ of populations, a z-test focuses on the _dispersion_ (deviation) of populations. \n",
    "- For example, if a teacher claims his students have an IQ of 112 based on a random sampling of 30 students, and the population IQ is 100 +/- 15, is his claim supported by statistical significance, or can he not reject the null hypothesis?\n",
    "    - To do this z-test, we obtain a z-value and compare it with a z-score found in a z-table. If the z-test value is higher than the z-table value, than the null hypothesis can be rejected.\n",
    "    - In this particular example, you use a p-value of 5%. That means you are looking at the dispersion of the last 5% in a normal distribution. If the teacher is claiming that his students are smarter than average (50% mark), then you look at the area between the average and the null hypothesis rejection area (the final 5%). In other words you look at the z-table for the 45% range and get a z-score. If the z-test score (defined by (sample avg - average of whole pop) / (standard dev of whole pop) / sqrt(n samples)) is higher than the z-table score, you can reject the null hypothesis.\n",
    "    <br> <br>\n",
    "    - Another example is having averages from two different tests, which have average scores and deviations for each score, and determine which test the student did better on? The higher z-score would answer that question. (sqrt(n) would be 1).\n",
    "    - A \"z-score\" value in itself means how far away the value is from the mean. If the value is 1.6, the datapoint is \"1.6 standard deviations to the right of the mean.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
