{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Notes\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Dispersion in a Population...\n",
    "\n",
    "Should I use __Standard Deviation__ or __Standard Error of the Mean?__\n",
    "\n",
    "_Standard deviation_ measures the overall dispersion of a _population_.\n",
    "\n",
    "_Standard Error of the mean_ measures the dispersion _in relation to a characteristics of a sample, such as a mean._ \n",
    "\n",
    "In other words, __you use Standard error of the mean for error bars/confidence intervals, as those things are usually aggregated _to a mean.___\n",
    "\n",
    "(SEM = stan dev/sqrt(num datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy, Precision, Recall, F-Score, R-Squared Score\n",
    "\n",
    "All different things to identify how _well_ a model works, but they all focus on different things:\n",
    "- __Accuracy__ is a measure of how \"right\" something is ((true positives + true negatives) / all datapoints)\n",
    "    - __R-Squared__ values are specific to regression, but means the same thing in a continuous variable context. R-squared is a measure of how much of the variance in a population is explained by the model. (1 - ((explained variance, as identified by sum of regression errors) / (total variance)))\n",
    "    \n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)\n",
    "<br>\n",
    "TP = \"A hit\"<br>\n",
    "TN = \"A rejection\"\n",
    "<br>\n",
    "FP = Type I error (\"False Alarm\") <br>\n",
    "FN = Type II error (\"A miss\")\n",
    "<br>\n",
    "- __Precision__ is a measure of the accuracy of _all positive values_ (Precision = TP / (TP + FP)). Stated another way, it's a measure of \"how many positives were predicted correctly.\"\n",
    "- __Recall__ (a/k/a \"Sensitivity\") is a measure of the accuracy of _the true positive values_ (Recall = TP / (TP + FN)). Stated another way, it's a measure of \"out of all positive predictions, how many of those were correct?\"\n",
    "    - The opposite of recall is __Specificity__, which measures how many of the negative predictions were correct negative predictions (Specificity = TN / (TN + FP))\n",
    "    - The __F-Score (or \"F1 Score\")__ is a combination of precision and recall: ((recall x precision) / (B^2 x recall + precision), where Beta is a coefficient to put weight on either recall or precision - the user's choice)\n",
    "        - A simple F1 score formula is: 2 x (recall x precision) / (recall + precision)\n",
    "        - Best F1 score == 1, worst == 0\n",
    "        \n",
    "- Determining the binarization point (for categorical classification problems) that maximizes sensitivity and specificity is via the __ROC curve.__ (See other notes for more explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Confusion_matrix.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization of above:\n",
    "from IPython.display import Image\n",
    "Image(url = 'https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals\n",
    "\n",
    "FOR A NORMAL DISTRIBUTION, there is a 95% chance that a  \"sample\" is within TWO STANDARD DEVIATIONS OF THE MEAN\n",
    "\n",
    "So to find a margin of error, we use +/- 2 x sigma (or 2 x standard error for sample populations)\n",
    "\n",
    "## Transformations\n",
    "\n",
    "(Covered more in Unit 2 and Unit 3 Notes)\n",
    "\n",
    "If data is not normally distributed, a lot of models will not yield accurate results, especially regression models (which require linearity and a number of other assumptions, including normality). To force normalization, you can do a __Box-Cox transformation__ via scipy. There are other transformations too, including exponential, root, and logarithmic transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# T-Test and Z-Tests\n",
    "\n",
    "The following computations are done with _one-tailed testing,_ where you are looking in a particular direction (e.g., is the IQ of a sample population _higher_ than the whole population?).\n",
    "\n",
    "## T-Test\n",
    "\n",
    "A t-test (also known as a _Student's t-Test_), is a measurement of hypothesis testing. It measures the distance between the mean of a sample population from the mean of another sample population. This distance is converted into a _t-value._ Referencing the t-value on a \"T-Chart.\" If a t-value is less than a p-value (usually < 0.05), then one can conclude that the sample two populations are statistically significant; you can reject the null hypothesis.\n",
    " - The __null hypothesis__ states: \"We cannot determine the reason why the data is organized in the way that it is: it is hypothesized that the organization is random.\" In finding statistical significance, we can _reject_ that statement and say that there is a _reason_ why the data is organized in the way that it is (making inferences and references to the given correlated features).\n",
    " \n",
    "__T-Value:__ (sample mean - nullhypothesis mean) / (sample population standard deviation / (sqrt(sample size 'n')))\n",
    " \n",
    "---\n",
    "\n",
    "## Z-Test\n",
    "\n",
    "Whereas a t-test focuses on the _means_ of populations, a z-test focuses on the _dispersion_ (deviation) of populations. \n",
    "- For example, if a teacher claims his students have an IQ of 112 based on a random sampling of 30 students, and the population IQ is 100 +/- 15, is his claim supported by statistical significance, or can he not reject the null hypothesis?\n",
    "    - To do this z-test, we obtain a z-value and compare it with a z-score found in a z-table. If the z-test value is higher than the z-table value, than the null hypothesis can be rejected.\n",
    "    - In this particular example, you use a p-value of 5%. That means you are looking at the dispersion of the last 5% in a normal distribution. If the teacher is claiming that his students are smarter than average (50% mark), then you look at the area between the average and the null hypothesis rejection area (the final 5%). In other words you look at the z-table for the 45% range and get a z-score. If the z-test score (defined by (sample avg - average of whole pop) / (standard dev of whole pop) / sqrt(n samples)) is higher than the z-table score, you can reject the null hypothesis.\n",
    "    <br> <br>\n",
    "    - Another example is having averages from two different tests, which have average scores and deviations for each score, and determine which test the student did better on? The higher z-score would answer that question. (sqrt(n) would be 1).\n",
    "    - A \"z-score\" value in itself means how far away the value is from the mean. If the value is 1.6, the datapoint is \"1.6 standard deviations to the right of the mean.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
