{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 - TensorFlow and Keras\n",
    "\n",
    "## 6.6.1 History of Artificial Neural Networks\n",
    "\n",
    "Most of the history is inapplicable to practical skills that these notes focus on. Basically, ANNs were not used until recently when GoogleBrain made it popular again. A key tool used by GoogleBrain was DistBelief, and its popularity rose due to its open source nature.\n",
    "\n",
    "In 2015, DistBelief was changed in a new iteration, becoming __TensorFlow.__ The competitor-tool to TensorFlow was the non-Google __Keras__. Nowadays, howver, Keras and TensorFlow are integrated, allowing Keras to be used natively with TensorFlow.\n",
    "\n",
    "Good introductory source: [A. Rethinavel Subramanian, _Int'l Journal of Engineering Research and Applications_, www.ijera.com, ISSN : 2248-9622, Vol. 4, Issue 1 (Version 2), January 2014, pp.237-241](https://www.academia.edu/5886469/AF4102237242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.2 How TensorFlow Works\n",
    "\n",
    "Tensors are array that have ___ranks.___ Ranks represent the number of dimensions of a tensor-array (or simply, a \"tensor\"). To convert a tensor to an actual NumPy array, use `.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main import\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3, 2, 1]                  # rank 1 (single dimensional vector)\n",
    "[[3, 2], [1, 3]]           # rank 2 (two dimensions)\n",
    "[[[1], [2]], [[1], [2]]]   # rank 3\n",
    "2                          # rank 0 (scalar values have no dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "\n",
    "__Thinkful Definition:__ \"Key object that is a place where things can happen in our model.\" Cool.\n",
    "\n",
    "#### Node - The \"Constant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_const = tf.constant(70)\n",
    "print(node_const) # notice how it prints the node itself, not the constant \"70\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Mathematical Operator\" (Add, Multiply, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_3:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_add = tf.add(node_const, node_const)\n",
    "\n",
    "print(node_add) # every time you re-run it, the name changes to Add_n+1\n",
    "# The value (0) doesn't change because the rank of the constant is always 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "node_place = tf.placeholder(tf.int32)\n",
    "\n",
    "print(node_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node - The \"Variable\"\n",
    "\n",
    "Has the properties of a placeholder (it literally \"holds the place of a value\") but with additional features, making it have a _variable value_ instead of a placeholder's constant value.\n",
    "\n",
    "Also, you need to manually initialize them (ugh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "q = tf.Variable([0], tf.float32)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions\n",
    "\n",
    "Instead of simply identifying the node, \"sessions\" actually \"runs\" the node and generates an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(node_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilizing all nodes as an example\n",
    "\n",
    "a = tf.placeholder(tf.int32)\n",
    "\n",
    "# Create an operator node that takes our placeholder and a constant node\n",
    "multiply_by_2 = tf.multiply(a, tf.constant(2))\n",
    "\n",
    "# Run the node to return our output\n",
    "sess.run(multiply_by_2, {a : 3}) # this is what you use a placeholder for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6,   8, 162],\n",
       "       [  4,  62,  26]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the beauty of tensors is that it performs higher matrix operations for you!\n",
    "# (something good for image data analysis? (wink hint wink?))\n",
    "sess.run(multiply_by_2, {a : [[3, 4, 81], [2, 31, 13]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.3 - Example TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting variables\n",
    "# Note that our initial value has to match the data type\n",
    "# so 1 would give an error since it's an int...\n",
    "b = tf.Variable([1.], tf.float32)\n",
    "m = tf.Variable([1.], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# Implement a linear model with shorthand for tf.add() by using '+'\n",
    "# and tf.multiply with '*'\n",
    "linear_model = m * x + b\n",
    "\n",
    "# New variables means we have to initialize again\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x:[1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.04001\n"
     ]
    }
   ],
   "source": [
    "# getting fancier...\n",
    "# creating a loss function and thereby determining accuracy\n",
    "\n",
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]}))\n",
    "\n",
    "# That's a high sum squared loss! Large error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9960036e-16\n"
     ]
    }
   ],
   "source": [
    "# You can go back and adjust the model with the node\n",
    "#########TF.ASSIGN!\n",
    "\n",
    "fixm = tf.assign(m, [-1.])\n",
    "fixb = tf.assign(b, [1.1])\n",
    "sess.run([fixm, fixb])\n",
    "print(sess.run(loss, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^The error is basically zero here, which is great!<br>\n",
    "But how did we know to re-assign m and b to be -1 and 1.1, respectively? Thinkful cheated...\n",
    "\n",
    "What we _can_ do is to create a gradient descent function to minimize the loss function. We can set the gradient descent to be a tensorflow node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[array([-0.9286998], dtype=float32), array([0.89036876], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Set your learning rate in Gradient Descent - 0.01 is just fine\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# reset values to incorrect defaults.\n",
    "# Otherwise, the \"session\" would work off of itself - VERY IMPORTANT!*******\n",
    "sess.run(init) \n",
    "\n",
    "# Loop for 100 iterations, trying to find optimal values\n",
    "for i in range(100):\n",
    "    sess.run(train, {x:[1, 2, 3, 4], y:[.1, -.9, -1.9, -2.9]})\n",
    "\n",
    "print(sess.run([m, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.4 Keras Introduction\n",
    "\n",
    "Keras is more accessible but can do less than TensorFlow (according to Thinkful, of course). Instead of TensorFlow's \"nodes\" and \"variables\", Keras has:\n",
    "\n",
    "__Layers:__ Like layers in a typical neural network model (a set of perceptron nodes).\n",
    "- A layer is called a \"dense\" layer if every node connects to every node in the next layer.\n",
    "\n",
    "__Models:__ The structure of your layering. You \"make a model\" by stacking the layers (see example below). Two flavors of model:\n",
    "- _Sequential_: Stacks of layers in a linear progression.\n",
    "- _Complex_: Non-sequential layering. Obviously such a model is more...ummm...'complex'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "\n",
    "model = Sequential()\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=100, input_dim=100)) # \"adding\" on top of one another \n",
    "model.add(Activation('relu'))              # forms a sequential model\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# ...I did it. Models need to be compiled and fit later\n",
    "# This will be demonstrated in the next section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.5 Keras MNIST Guided Example \n",
    "\n",
    "__Goal:__ Using the MNIST dataset, we will use neural networks to classify handwritten numbers as the proper digits, 0-9. Note that this data is sparse!!!!!\n",
    "\n",
    "MNIST is a good example of neural network usage. Multiple layers full of values allows for good unsupervised analysis. __To use neural networks, it needs to be fed A LOT of data__ (albeit, a lot of sparse data :( ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "from keras.datasets import mnist # easy to load!!!!!!!!\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() # split and load data\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(28*28)        # MNIST picture resolution\n",
    "print(len(x_train)) # array size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# need to compress/shape the data into flat vectors for each digit\n",
    "\n",
    "# Change shape \n",
    "# Need to reshape 60,000 arrays to length 784, one for each image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, \n",
    "# create 10 binary columns (one for each number!!!!)\n",
    "from keras.utils.np_utils import to_categorical \n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiating our Keras model\n",
    "'''\n",
    "Keras model will use \"dense layers\" and \"dropouts\"\n",
    "\n",
    "***Dense layers*** - layers that are fully connected\n",
    "***Dropout Drops*** - drops out a fraction of the perceptrons to prevent overfitting\n",
    "'''\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "'''\n",
    " Add dense layers to create a fully connected MLP\n",
    " Note that we specify an input shape for the first layer, but\n",
    "***ONLY*** the first layer!!!!!!!!!\n",
    "'''\n",
    "\n",
    "# Relu = \"Rectified Linear Unit (standard activation function)\"\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.4167 - acc: 0.8776 - val_loss: 0.2037 - val_acc: 0.9397\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2020 - acc: 0.9394 - val_loss: 0.1379 - val_acc: 0.9586\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1558 - acc: 0.9541 - val_loss: 0.1200 - val_acc: 0.9639\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1325 - acc: 0.9603 - val_loss: 0.1057 - val_acc: 0.9681\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1149 - acc: 0.9655 - val_loss: 0.1021 - val_acc: 0.9686\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1051 - acc: 0.9684 - val_loss: 0.0933 - val_acc: 0.9709\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0963 - acc: 0.9706 - val_loss: 0.0894 - val_acc: 0.9742\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0894 - acc: 0.9728 - val_loss: 0.0884 - val_acc: 0.9731\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0846 - acc: 0.9742 - val_loss: 0.0879 - val_acc: 0.9747\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0788 - acc: 0.9758 - val_loss: 0.0878 - val_acc: 0.9749\n",
      "Test loss: 0.08778246227339842\n",
      "Test accuracy: 0.9749\n"
     ]
    }
   ],
   "source": [
    "# fitting the instantiated model\n",
    "'''\n",
    "BATCH_SIZE = number of samples to use in each step \n",
    "----larger size = faster (bigger steps) but \n",
    "----decreases accuracy (learning rate not as small step-wise)\n",
    "\n",
    "----Notice how the layers added above add up to 128 (batch size?)\n",
    "----layers are size=64 (see above) 2^x size layers good for parallized computation\n",
    "\n",
    "EPOCH = essentially an iteration of the model - \n",
    "-----each epoch improves on what was learned in the previous iteration/epoch\n",
    "'''\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,     # essentially iterations of the model going through the data\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Convolutional Neural Networks\n",
    "\n",
    "__Convolution:__ Analyzing data via overlapping segments of a given feature upon which it develops its model.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.Image object>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1200/1*GcI7G-JLAQiEoCON7xFbhg.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://cdn-images-1.medium.com/max/1200/1*GcI7G-JLAQiEoCON7xFbhg.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1000/1*yHKCrrgpdewt30JcE7016g.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://cdn-images-1.medium.com/max/1000/1*yHKCrrgpdewt30JcE7016g.png')\n",
    "\n",
    "# more good info: https://www.peculiar-coding-endeavours.com/2018/mlp_vs_cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEPS TO CONVOLUTION__\n",
    "1. Define shape of input data (most often, first chunk of code is reshaping the loaded data)\n",
    "2. Create the tiles (or ___kernels___)\n",
    "3. Reduce the sample size into a pooling layer (a process called ___downsampling___)\n",
    "4. Flatten the downsampled data, place flattened data into dense layers and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "[5 0 4 ... 5 6 8]\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ebb0fb0c58f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m    \u001b[1;31m#       validation_data=(x_test, y_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m#score = model.evaluate(x_test, y_test, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. LOAD DATA, SHUFFLE, AND RESHAPE\n",
    "\n",
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "# K is \"backend\" work...don't ask\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(y_train)\n",
    "print(type(y_train))\n",
    "\n",
    "#---------------------------------------------------\n",
    "# 2. MODEL INSTANTIATION\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 3. MODEL FIT & RUN\n",
    "\n",
    "#model.fit(x_train, y_train,\n",
    "#          batch_size=128,\n",
    " #         epochs=10,\n",
    "  #        verbose=1,\n",
    "   #       validation_data=(x_test, y_test))\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# convolutional NNs take A LONG LONG TIME to run, so be patient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the convolutional model takes much longer...but it yielded a higher accuracy than the previous model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Recurrent Neural Networks\n",
    "\n",
    "Recurrent NNs are different from the above NNs, which are all __feed-forward__ - that is, data flows in one direction until it reaches the end.\n",
    "\n",
    "Recurrent NNs instead \"cycle\" through the network. Accordingly, \"Sequential models\" are no longer an option. Also, this is more complex than feed-forward, meaning it will take even longer (yay...)!\n",
    "\n",
    "Example of a Recurrent Neural Network is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 1.0018 - acc: 0.6530 - val_loss: 0.6586 - val_acc: 0.7763\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.4721 - acc: 0.8445 - val_loss: 0.3947 - val_acc: 0.8691\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.3060 - acc: 0.9035 - val_loss: 0.2397 - val_acc: 0.9229\n",
      "Test loss: 0.23966183296740054\n",
      "Test accuracy: 0.9229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters.\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3      # notice this is much lower than the 10 - \n",
    "                # don't wanna take forever!\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "#------------------------------------------------------------\n",
    "\n",
    "# The data, shuffled, and split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper. - THIS IS THE BASIS OF THE LSTM RNN!!!!!\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW\n",
    "\n",
    "[This is a good example of how to do a CNN Keras model](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6).\n",
    "\n",
    "__Steps to do a Keras Model:__\n",
    "1. Do basic imports (including various Keras imports) and import data.\n",
    "\n",
    "\n",
    "2. Split and clean the data.\n",
    "\n",
    "\n",
    "3. Normalize the data _values_ (i.e., change pixel values to \\[0,1\\] as neural network converges faster that way).\n",
    "    - In the MNIST dataset, pixel values range from 0 to 255, depending on the pixel's darkness level. To normalize this data, simply divide the features columnwise: \n",
    "        - `X_train = [['pixel_1', 'pixel_2, ... pixel_784]]`\n",
    "        - `X_train = X_train / 255`\n",
    "\n",
    "\n",
    "4. Reshape the data (flatten it to make it an input that's appropriate for a Keras model)\n",
    "    - `X_train = X_train.values.reshape(-1, 28, 28, 1)` representing 28x28 pixels and 1 for 'canal' dimension\n",
    "    - For different neural networks, reshaping syntax may need to be altered (see above examples)\n",
    "    - If target variable is not encoded into numbers, make sure to do that\n",
    "    - Not only do __YOU HAVE TO__ one-hot-encode the values, but you __ALSO HAVE TO__ create/dummy columns representing each different categorical value (as opposed to having a one-dimensional array of all the values!!!!) \n",
    "        - You can do the above via `from keras.utils.np_utils import to_categorical` and later `Y_train = to_categorical(Y_train, num_classes = 10)`. It's Keras law.\n",
    "\n",
    "\n",
    "5. Setting up the Keras model architecture:\n",
    "    - adding neural network layers:\n",
    "        - `model = Sequential()`\n",
    "        - Add as many layers as you want: `model.add(`_(type of NN layer)_`)\n",
    "        - After adding desired layers, do `model.add(Flatten())` and then `model.add(Dense(...))` and `model.add(Dropout())`\n",
    "    - Modify model for efficiency:\n",
    "        - Add optimizer (adjusts model's learning rate)\n",
    "            - `optimizer = RMSprop(...)`\n",
    "            - `model.compile(optimizer=optimizer, loss= ..., metrics=['accuracy']`\n",
    "        - Add annealer (speeds up convergence by adjusting the learning rate if the model is learning too slow or too fast)\n",
    "            - `learningratereduction = ReduceLROnPlateau(...)`\n",
    "        \n",
    "        \n",
    "6. Set up Data augmentation (if the model runs into an overfitting problem, otherwise, ignore)\n",
    "\n",
    "\n",
    "7. Fit the model (and adjust parameters, including the annealer in \"callbacks=\" parameter if necessary)\n",
    "\n",
    "\n",
    "8. Test model's accuracy.\n",
    "\n",
    "\n",
    "9. If using a supervised dataset, use a confusion matrix for individual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible errors:\n",
    "\n",
    "1. Error when calling \"cross_entropy\" \n",
    "    - __Answer:__ shaping error when analyzing the target variable. You need to do y_train (or test) = `keras.utils.to_categorical(y_train, num_classes= (num of categories in target variable))\n",
    "    \n",
    "    \n",
    "2. Array of size __X__ cannot be reshaped using (your dimensions)\n",
    "    - __Answer:__ Your dimensions do not equal the dimensions of the original dataset. If you are having trouble figuring out what dimensions will work, reshape to the original dimensions of the dataset (yes, it is still a \"reshaping\" in the eyes of Keras)\n",
    "\n",
    "\n",
    "3. Layer dense_21 expected (1,) but got (10,)\n",
    "    - __Answer:__ As shown in the above example, the final layer must be the same dimensionality as that of the target variable. If appropriately one-hot encoded (see Possible Error 1 above), then the number of categories encoded to will be the number found in the final model.add(Dense(# here)) layer when creating the Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Specific Neural Network to Use?\n",
    "\n",
    "__source:__ [\"When to use MLP, CNN, and RNN Neural Networks\", Jason Brownlee, Jul. 23, 2018](https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/)\n",
    "\n",
    "Three main types of ANNs:\n",
    "1. __Multi-layer Perceptrons (\"Classical\" or \"Feed forward\" network):__ You can use sklearn's version of this. Use this one for:\n",
    "    - Classification prediction problems\n",
    "    - Regression prediction problems\n",
    "    - Best with __tabular data__ (as opposed to image, audio, or other medium of data)\n",
    "        - If working with non-tabular data, you should still use this ANN for \"baseline\" prediction values. This is important to avoid overfitting of more complex ANNs!!!!!!!\n",
    "        \n",
    "2. __Convolutional Neural Networks (CNNs):__ These models develop an internal representation of a 2D image. That representation is then positioned and scaled for analysis. Use this one for:\n",
    "    - Classification prediction problems\n",
    "    - Regression prediction problems\n",
    "    - Image data\n",
    "        - The reason this is designed for image data is because the model is good at constructing a __\"spatial relationship\"__ with the internal representation. This makes them good not just for image data, but for (1) word-order relationship in text and (2) the ordered relationship in steps of a time-series.\n",
    "        \n",
    "3. __Recurrent Neural Networks (aka \"Feedback\" networks)__: Uses a \"non-linear\" approach, where the inputs of a layer are fed back in, and the data can go essentially in _any_ direction within the network. This makes them good for __sequence prediction__ problems, where each data observation might have \"multiple steps\". The __\"Long Short-Term Memory\" (LSTM)__ RNN is a successful RNN because it significantly speeds up the training process.\n",
    "    - Text data\n",
    "    - Classification prediction problems\n",
    "    - Regression prediction problems\n",
    "    - Speech data\n",
    "    - Generative models\n",
    "    - Do __NOT__ use RNNs for:\n",
    "        - tabular data\n",
    "        - image data\n",
    "        - MLPs or even simple classification/regression models fare better at working with tabular and image data than  RNNs do. \n",
    "   \n",
    "### BUT WHY CHOOSE ONE WHEN YOU CAN HYBRIDIZE THEM ALL TOGETHER???\n",
    "   \n",
    "4. __CNN LTSM Architecture:__ This model starts with CNN layers at the input, LSTM in the middle, then MLP at the output. Such a model would be perfect for _video data_ (image analysis, then sequential analysis, then classification analysis).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1000/1*S7Q2Rh7ba0jW5pQJovekSw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# good example of the various things you can do.\n",
    "# Again, this all depends on (1) THE DATA and (2) THE DESIRED OUTPUT\n",
    "Image(url='https://cdn-images-1.medium.com/max/1000/1*S7Q2Rh7ba0jW5pQJovekSw.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good miscellaneous things you can add for analysis purposes:\n",
    "\n",
    "# Look at confusion matrix for categorical things\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0) # see above for Y_true and Y_pred_classes\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap-up conditions\n",
    "\n",
    "# predict results\n",
    "results = model.predict(test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results1 = np.argmax(results, axis=1)\n",
    "\n",
    "results2 = pd.Series(results1, name=\"Label\")\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1,28001), name=\"ImageId\"), results2], axis=1)\n",
    "\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
