{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Machine Learning Readability\n",
    "\n",
    "The following are techniques to establish how to read results of models and how to figure out the features to get there.\n",
    "\n",
    "## 1. Permutation Importance\n",
    "\n",
    "Permutation Importance is a simple sklearn way of identifying the best features. This is good if the data isn't intuitive or you don't have proper labels on your data - that way, you can work with the highest correlated features and play with them to get even better features.\n",
    "\n",
    "Permutation importances are done __after a model has been fit!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain feature importance via \"eli5\" sklearn library\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_features = ['pickup_longitude',\n",
    "                 'pickup_latitude',\n",
    "                 'dropoff_longitude',\n",
    "                 'dropoff_latitude',\n",
    "                 'passenger_count']\n",
    "\n",
    "# fit model first!\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "first_model = RandomForestRegressor(n_estimators=30, \n",
    "                                    random_state=1)\n",
    "first_model.fit(train_X, train_y)\n",
    "\n",
    "# permutation importance requires a second fit...but gotta separately fit the model first\n",
    "\n",
    "perm = PermutationImportance(first_model, random_state=1).fit(val_X, val_y)\n",
    "\n",
    "eli5.show_weights(perm, feature_names = base_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Partial Dependence Plots\n",
    "\n",
    "Feature Importance above shows _what_ variables most affect predictions...but it doesn't show _how_ a feature affects a prediction.\n",
    "\n",
    "That's where __Partial Dependence Plots__ come in. Like Permutations, PD plots are calculated __after__ a model has been fit.\n",
    "\n",
    "__How it Works:__ You alter the value of _one feature_ after a model has been fit and see the change in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier(random_state=0, \n",
    "                                    max_depth=5, \n",
    "                                    min_samples_split=5).fit(train_X, train_y)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'Goal Scored')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PDP Graph: \n",
    "- x axis is the feature values (price, goals scored, etc.)\n",
    "- y axis is the change in prediction from what it would be predicted at the baseline (leftmost of the graph) value. Shaded area is a confidence interval.\n",
    "\n",
    "PDP Graphs only analyze one feature. However, you can change it to account for changes in _two_ features in a quasi-heatmap graph, where the heat is the degree of change. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2 feature PDP graph is similar to regular PDP plot except we use pdp_interact \n",
    "instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
    "'''\n",
    "features_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\n",
    "inter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n",
    "\n",
    "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SHAP Values\n",
    "\n",
    "__SHAP:__ \"SHapley Additive exPlanations\" breaks down a prediction and shows the impact of _each_ feature. The sum of all SHAP values is the total predicted change from baseline.\n",
    "\n",
    "Good examples for looking into SHAP values are:\n",
    "- A model says a bank shouldn't loan someone money, and the bank is legally required to provide a basis for the loan rejection.\n",
    "\n",
    "- A healthcare provider wants to identify the factors that are driving the patient's risk of some disease, so that those factors can be targeted.\n",
    "\n",
    "Example of SHAP value code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "\n",
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[row_to_show]  \n",
    "# use 1 row of data here. Could use multiple rows if desired\n",
    "\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "\n",
    "my_model.predict_proba(data_for_prediction_array)\n",
    "\n",
    "########################################################\n",
    "\n",
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "# Also DeepExplainer for Deep Learning models and\n",
    "# KernelExplainer for all other models (although generally slower and offers only an approximation)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "# The above variable creates two arrays - feature by feature probability of a negative outcome\n",
    "# and feature by feature probability of a positive outcome\n",
    "\n",
    "########################################################\n",
    "# An easier way to understand the above SHAP value arrays is to visualize them\n",
    "# Below is the code for that:\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
