{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Algorithms and Data Structures\n",
    "\n",
    "## 5.1.1 Basic Data Structures\n",
    "\n",
    "1. __The List__ <br>\n",
    "    a. Ordered sequence of items with an incrementing index (which starts at 0). <br>\n",
    "    b. ___Good because___ we can call any element by simply calling its index. This also makes adding things to the end of a list easy. <br>\n",
    "    c. ___Bad because___ removing something from a list messes up the entire index, because the list _must_ start at 0.\n",
    "    \n",
    "    \n",
    "2. __Linked List__ <br>\n",
    "    a. Maintained order of items _but_ no index - each entry is \"linked\" to the next item. <br>\n",
    "    b. ___Good for___ deletion of data but ___bad for___ accessing items of data.\n",
    "    \n",
    "    \n",
    "3. __Queue__ (\"FIFO\" organization) <br>\n",
    "    a. Specialized linked list where items _can only come in one end and out the other._ (i.e., consisting of \"enqueing items\" and \"dequeing items.\") <br>\n",
    "    b. Huge drawbacks with accessing and searching, as you have to go through the queue sequentially to see if an element is present (resulting in O(n) efficiency).\n",
    "\n",
    "\n",
    "4. __Stack__ (\"LIFO\" organization) <br>\n",
    "    a. Similar drawbacks to the queue, but like the queue, it is useful in certain circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Sorting Items\n",
    "\n",
    "These are algorithms that have different methods to sort a list of items.\n",
    "\n",
    "1. __Insertion Sort__ <br>\n",
    "    a. Take an item, move it to the sorted spot and push the associated number of items up or down in a list. <br>\n",
    "    b. O(n^2) timing, where n = length of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random \n",
    "\n",
    "random.seed(a=100)\n",
    "\n",
    "short_list = list(random.sample(range(1000000), 10))\n",
    "long_list = list(random.sample(range(1000000), 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sort(input_list):\n",
    "    # Copy the input to a new list so we don't modify the original.\n",
    "    new_list = input_list\n",
    "    \n",
    "    # Iterate through the list.\n",
    "    for i in range(len(new_list)):\n",
    "        # Assign place to a new variable.\n",
    "        j = i\n",
    "        \n",
    "        # Move through the list as long as the previous position is larger\n",
    "        # than the current element of list.\n",
    "        while j > 0 and new_list[j - 1] > new_list[j]:\n",
    "            \n",
    "            # Swap places.\n",
    "            new_list[j - 1], new_list[j] = new_list[j], new_list[j - 1]\n",
    "            \n",
    "            # Reduce j by one.\n",
    "            j = j - 1\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n",
      "[152745, 183236, 366725, 412125, 477025, 481850, 739784, 767514, 808225, 997948]\n",
      "\n",
      "--- 8.67147421836853 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# checking how long an insertion sort takes\n",
    "# Start the timer.\n",
    "start_time = time.time()\n",
    "\n",
    "# Run our insertion sort.\n",
    "insert_sort(short_list)\n",
    "\n",
    "# Print time to show runtime.\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(insert_sort(short_list))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "insert_sort(long_list)\n",
    "\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Merge Sort__ <br>\n",
    "    a. Divides and conquers the list by breaking up the list into parts, sorting those parts, and re-combining the list. <br>\n",
    "    b. Much faster: O(n(log(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our merge function takes two ordered lists and merges them together into one ordered list\n",
    "\n",
    "def merge(a, b):\n",
    "    # Check for empty list.\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return a or b\n",
    "    \n",
    "    # Start with an empty result.\n",
    "    result = []\n",
    "    # Track two indexes.\n",
    "    i, j = 0, 0\n",
    "    # Set a while condition to ensure we iterate only for the length of our two lists.\n",
    "    while (len(result) < len(a) + len(b)):\n",
    "        # If a's next element is lower append that element to our result.\n",
    "        if a[i] < b[j]:\n",
    "            result.append(a[i])\n",
    "            i += 1\n",
    "        # Otherwise append b's next element.\n",
    "        else:\n",
    "            result.append(b[j])\n",
    "            j += 1\n",
    "        # When one list is empty just append everything from the other list and stop.\n",
    "        if i == len(a) or j == len(b):\n",
    "            result.extend(a[i:] or b[j:])\n",
    "            break \n",
    "\n",
    "    return result\n",
    "\n",
    "def merge_sort(lst):\n",
    "    if len(lst) < 2:\n",
    "        return lst\n",
    "\n",
    "    mid = int(len(lst) / 2)\n",
    "    a = merge_sort(lst[:mid])\n",
    "    b = merge_sort(lst[mid:])\n",
    "\n",
    "    return merge(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n",
      "[152745, 183236, 366725, 412125, 477025, 481850, 739784, 767514, 808225, 997948]\n",
      "\n",
      "--- 0.07773399353027344 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Test on short list.\n",
    "# Start timer.\n",
    "start_time = time.time()\n",
    "\n",
    "# Run our insertion sort.\n",
    "merge_sort(short_list)\n",
    "\n",
    "# Print time to show runtime.\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(merge_sort(short_list))\n",
    "# Test on long list.\n",
    "start_time = time.time()\n",
    "\n",
    "merge_sort(long_list)\n",
    "\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# compare the above to python cheating way: \"sorted(list_to_sort)\"\n",
    "\n",
    "# Start Timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Sort the default list. Note that .sort() will sort in place, which would alter default_list.\n",
    "sorted(long_list)\n",
    "\n",
    "# Print time to show runtime\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 Trees\n",
    "\n",
    "Another algorithm to be explored. Top down, beginning with the _root_, from the _parent_ root node, there are _children_ nodes. If the node does not have any children, it is a terminal _leaf_ node.\n",
    "\n",
    "The benefit to a tree is that, unlike lists, they are much more flexible. There can be two, three, or more children per node.\n",
    "\n",
    "__When to use Tree Algorithms:__\n",
    "- Hierarchical data (departments in an agency, models and their parts, etc.)\n",
    "\n",
    "A subset of trees is a __heap.__ This is a tree where a parent is _always_ a higher value than its children.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: [a0]\n",
      "Node: [b0]\n",
      "Node: [b1]\n",
      "Node: [b2]\n",
      "Node: [c0]\n",
      "Node: [c1]\n",
      "Node: [d0]\n",
      "\n",
      "Node: [a0]\n",
      "Node: [b0]\n",
      "Node: [c0]\n",
      "Node: [d0]\n",
      "Node: [c1]\n",
      "Node: [b1]\n",
      "Node: [b2]\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE OF A TREE\n",
    "\n",
    "\"\"\"\n",
    "GOAL: Implement a binary tree, which is filled with 15 pieces of random data. \n",
    "Your job is to then write a program to traverse the tree using a breadth first traversal. \n",
    "If you want additional practice, try other forms of traversal.\n",
    "\n",
    "recipe_simple_tree_traversal.py\n",
    "\n",
    "Simple breadth-first and depth-first tree traversal for any tree.\n",
    "\n",
    "The recipe is contained within the first two functions. The rest is\n",
    "a test bed for demonstration.\n",
    "\"\"\"\n",
    "__author__ = \"Jack Trainor\"\n",
    "__date__ = \"2015-12-16\"\n",
    "\n",
    "import sys\n",
    "    \n",
    "def get_breadth_first_nodes(root):\n",
    "    nodes = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        cur_node = stack[0]\n",
    "        stack = stack[1:]\n",
    "        nodes.append(cur_node)\n",
    "        for child in cur_node.get_children():\n",
    "            stack.append(child)\n",
    "    return nodes\n",
    "\n",
    "def get_depth_first_nodes(root):\n",
    "    nodes = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        cur_node = stack[0]\n",
    "        stack = stack[1:]\n",
    "        nodes.append(cur_node)        \n",
    "        for child in cur_node.get_rev_children():\n",
    "            stack.insert(0, child)\n",
    "    return nodes\n",
    "\n",
    "########################################################################\n",
    "class Node(object):\n",
    "    def __init__(self, id_):\n",
    "        self.id = id_\n",
    "        self.children = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Node: [%s]\" % self.id\n",
    "    \n",
    "    def add_child(self, node):\n",
    "        self.children.append(node) \n",
    "    \n",
    "    def get_children(self):\n",
    "        return self.children         \n",
    "    \n",
    "    def get_rev_children(self):\n",
    "        children = self.children[:]\n",
    "        children.reverse()\n",
    "        return children         \n",
    "\n",
    "########################################################################\n",
    "def println(text):\n",
    "    sys.stdout.write(text + \"\\n\")\n",
    "    \n",
    "def make_test_tree():\n",
    "    a0 = Node(\"a0\")\n",
    "    b0 = Node(\"b0\")      \n",
    "    b1 = Node(\"b1\")      \n",
    "    b2 = Node(\"b2\")      \n",
    "    c0 = Node(\"c0\")      \n",
    "    c1 = Node(\"c1\")  \n",
    "    d0 = Node(\"d0\")   \n",
    "    \n",
    "    a0.add_child(b0) \n",
    "    a0.add_child(b1) \n",
    "    a0.add_child(b2)\n",
    "    \n",
    "    b0.add_child(c0) \n",
    "    b0.add_child(c1) \n",
    "    \n",
    "    c0.add_child(d0)\n",
    "    \n",
    "    return a0                  \n",
    "\n",
    "def test_breadth_first_nodes():\n",
    "    root = make_test_tree()\n",
    "    node_list = get_breadth_first_nodes(root)\n",
    "    for node in node_list:\n",
    "        println(str(node))\n",
    "\n",
    "def test_depth_first_nodes():\n",
    "    root = make_test_tree()\n",
    "    node_list = get_depth_first_nodes(root)\n",
    "    for node in node_list:\n",
    "        println(str(node))\n",
    "\n",
    "########################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    test_breadth_first_nodes()\n",
    "    println(\"\")\n",
    "    test_depth_first_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: [a0]\n",
      "Child is   Node: [b0]\n",
      "Child is   Node: [b1]\n",
      "Child is   Node: [b2]\n",
      "Node: [b0]\n",
      "Child is   Node: [c0]\n",
      "Child is   Node: [c1]\n",
      "Node: [b1]\n",
      "Node: [b2]\n",
      "Node: [c0]\n",
      "Child is   Node: [d0]\n",
      "Node: [c1]\n",
      "Node: [d0]\n",
      "Node: [a0]\n",
      "Node: [b0]\n",
      "Node: [b1]\n",
      "Node: [b2]\n",
      "Node: [c0]\n",
      "Node: [c1]\n",
      "Node: [d0]\n"
     ]
    }
   ],
   "source": [
    "def get_breadth_first_nodes(root):\n",
    "    nodes = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        cur_node = stack[0]\n",
    "        print(cur_node)\n",
    "        stack = stack[1:]\n",
    "        nodes.append(cur_node)\n",
    "        for child in cur_node.get_children():\n",
    "            print('Child is  ', child)\n",
    "            stack.append(child)\n",
    "    return nodes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_breadth_first_nodes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Scraping\n",
    "\n",
    "__Scraping__ is the act of extracting data from a website and saving it to a file (or database). A __scraper__ is the function that does that.\n",
    "\n",
    "(In practicality, most employers don't care about scraping because they have _too much data_).\n",
    "\n",
    "## 5.2.1 What is scraping?\n",
    "\n",
    "Three step process:\n",
    "1. Access webpages\n",
    "    - This is through python, not through a web browser\n",
    "2. Locate the desired information/data from those webpages\n",
    "    - This is called __parsing__ the data - pulling out specific information you want.\n",
    "3. Save that information in a different location\n",
    "    - save to local terminal or dump into SQL database\n",
    "    \n",
    "__Scraping Python Libraries:__\n",
    "- _Requests_ (for web pages)\n",
    "- _BeautifulSoup_ or _lxml_ (for paring)\n",
    "- _JSON_ or _pymysql_ (for storage)\n",
    "\n",
    "## 5.2.2 Scrapy\n",
    "\n",
    "[Scrapy](https://doc.scrapy.org/en/latest/topics/api.html) is a widely used scraper. The user provides one or more starting URLs, and the scraper sends a request to the server for the information at that URL. The server provides the information requested (or an error code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: UserWarning: You do not have a working installation of the service_identity module: 'cannot import name 'verify_ip_address' from 'service_identity.pyopenssl' (C:\\Anaconda\\lib\\site-packages\\service_identity\\pyopenssl.py)'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n",
      "2019-05-01 18:55:40 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n",
      "2019-05-01 18:55:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1a  20 Nov 2018), cryptography 2.4.2, Platform Windows-10-10.0.17763-SP0\n",
      "2019-05-01 18:55:40 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-05-01 18:55:41 [scrapy.extensions.telnet] INFO: Telnet Password: 65cd151eb269e42c\n",
      "2019-05-01 18:55:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-01 18:55:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-01 18:55:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-01 18:55:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-01 18:55:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-01 18:55:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-01 18:55:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2019-05-01 18:55:42 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://everydaysexism.com/> from <GET http://www.everydaysexism.com>\n",
      "2019-05-01 18:55:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://everydaysexism.com/> (referer: None)\n",
      "2019-05-01 18:55:42 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-01 18:55:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 438,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 11679,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 1, 22, 55, 42, 855371),\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 8,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2019, 5, 1, 22, 55, 41, 826587)}\n",
      "2019-05-01 18:55:42 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# designing a basic scraper with scrapy\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.everydaysexism.com', # sample URL here\n",
    "    ]\n",
    "\n",
    "    # What to do with the URL.  Here, we tell it to download all the code and save\n",
    "    # it to the mainpage.html file\n",
    "    def parse(self, response):\n",
    "        with open('mainpage.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "\n",
    "\n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUICK NOTE:\n",
    "\n",
    "The kernel for scrapy needs to be restarted ___each time you run it.___ If you don't restart the kernel, you'll get a \"ReactorNotRestartable\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-df5af79b660a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Start the crawler with our spider.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mESSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Success!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scrapy\\crawler.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, stop_after_crawl)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shutdown'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1272\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \"\"\"\n\u001b[0;32m   1250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m         \u001b[0mReactorBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.everydaysexism.com',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.********\n",
    "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading what was scraped into the json file - placing it in a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "firstpage = pd.read_json('firstpage.json', orient='records')\n",
    "print(firstpage.shape)\n",
    "print(firstpage.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis only scraped a single webpage. If you want to scrape _all of the pages_ of a website, scrapy will have to call itself via a recursive algorithm. Scrapy will identify what pages are connected to the webpage being scraped, and it will scrape those pages too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 Breaking the Website\n",
    "\n",
    "The speed at which scrapers access a page may cause the website to crash. In fact, you may manually adjust the scraper parameter to introduce _delays_ in between calls to the web server. Other ways to minimize crashing the site include:\n",
    "\n",
    "- _Autothrottling_, which dynamically sets the delay based on how quickly the server responds to scrapy's requests.\n",
    "- _Caching_, where scrapy will use a cached copy of the website (found on your terminal) instead of calling the exact page from the website _again_.\n",
    "\n",
    "__Robots__\n",
    "for every website, there is a \"robots\" file that tells scrapy's which portions of the sites are up for grabs and which are off-limits. For example, Amazon's is: https://amazon.com/robots.txt. If you scrape something that is forbidden: scrapy's traceback will indicate `[scrapy] DEBUG: Forbidden by robots.txt:<GET http://website.com/login>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4 Application Programming Interfaces (APIs)\n",
    "\n",
    "Large companies use an API in lieu of their actual websites; in other words, you access the data from the API, not from manually scraping the website. This makes it easier to obtain the website's information, and at the same time it doesn't clog up the website with scraping.\n",
    "\n",
    "__Accessing an API__\n",
    "Well, you need an _API key_ or _API token_.\n",
    "\n",
    "__The Basics of an API__\n",
    "Users have __Access__ via a key. There they make __requests__ to the API (the data you want with a call to the API), and the API provides a __response__, which is the return of data to the API (usually in .json format).\n",
    "\n",
    "#### Example API - Wikipedia API\n",
    "\n",
    "Sometimes, you still need a scrapy even when working with an API. This is because the API will only provide a limited number of responses (which is intentional to prevent server overloading). A scrapy would go ahead and make multiple calls _to the API._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4c2dc776e969>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Starting the crawler with our spider.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWikiSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'First 100 links extracted!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scrapy\\crawler.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, stop_after_crawl)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shutdown'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1272\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \"\"\"\n\u001b[0;32m   1250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m         \u001b[0mReactorBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good links to use for basic understanding (xpath is a bitch and a half)\n",
    "- https://bigtheta.io/2016/02/08/web-scraping-in-python.html\n",
    "- https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3\n",
    "- https://python.gotrained.com/scrapy-tutorial-web-scraping-craigslist/\n",
    "\n",
    "__Also BeautifulSoup instead of Scrapy????__\n",
    "\n",
    "Much easier to work with. All of the CSS and path-finding is embedded in the program. You can do more with scrapy (more malleable), but greater customization comes greater complexity.\n",
    "\n",
    "http://complx.me/2016-08-25-scrapy-guide/\n",
    "\n",
    "### FEEL FREE TO USE ANY OF THE SCRAPERS FOUND WITHIN THOSE LINKS!!!!\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Big Data\n",
    "\n",
    "Big data involves different architectures that can accommodate large data recording and storage sets.\n",
    "\n",
    "## 5.3.1 Big Data Definition\n",
    "\n",
    "__Big Data:__ Generally, it's the kind of data that cannot be analyzed on a local machine (millions to billions of rows, _at least_ gigabytes of data - sometimes terabytes or larger...).\n",
    "\n",
    "Accessing this kind of data requires certain frameworks (like Hadoop).\n",
    "\n",
    "## 5.3.2 Hadoop and Big Data Storage\n",
    "\n",
    "Hadoop is the primary way of accessing big data. Primarily, Hadoop has a core component called the __Hadoop distributed file system__, where the data is \"_distributed_\" across a number of machines instead of just one big drive. \n",
    "\n",
    "Similarly, there is a second component called __MapReduce__, which is a data processing tool for distributed systems (such as Hadoop). The querying language for accessing this data depends upong the type of _querying layer_, which is often \"__Hive__\" (and to do the queries, you do HiveQL, although there are other more efficient querying languages dealing with Hive such as Presto or Redshift).\n",
    "\n",
    "## 5.3.4 Distributed Computing and Spark\n",
    "\n",
    "So how does distributed computing actually 'work'?\n",
    "\n",
    "On a local machine, you can _parallelize_ via multiple CPU cores. This means the computer is performing two separate calculations simultaneously. If a model is across two parallelized cores, the processing time is cut in half. Some models parallelize well (e.g., random forest, which just does splits up the tree jobs across cores) and others not so much (e.g., SVMs).\n",
    "\n",
    "On distributed machinery, you would use __Spark.__ It is specifically designed to work with Hadoop. Python code translates well into Spark via __PySpark__ and Spark versions of iPython notebooks and sci-kit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Survey Design\n",
    "\n",
    "This section involves how to create good surveys, including accounting for the limitations and biases found in self-reported data and the financial and temporal limitations of surveys in general.\n",
    "\n",
    "## 5.4.1 Question Design\n",
    "\n",
    "__Question Types:__\n",
    "- Multiple choice (choose one) - here, options are mutually exclusive \n",
    "- Multiple choice (select all that apply - not mutually exclusive)\n",
    "- Write-in/Free response\n",
    "- Yes/No questions\n",
    "- Rating scale questions\n",
    "- Ranking questions\n",
    "- Constant sum (e.g., \"Divide 100 points among the following items, representing the percentage of time on the given task. All points must total 100.\")\n",
    "\n",
    "For all the above, the possible answers must encompass the entire range of possible answers (i.e., \"What is your favorite color?\" may not be suited for multiple choice, unless you plan on listing a lot of different colors).\n",
    "- Then again, you can always get around this by listing \"Other\" as an option).\n",
    "\n",
    "__Flawed Questions:__\n",
    "Questions may be \"loaded\" in terms of how the question is framed. (1) \"Should we allow dancing on Sundays\" and (2) \"Should we forbid dancing on Sundays\" logically pertains to the same thing, and therefore should yield the same results, but __people are much more comfortable refusing a gain ((1)) than embracing a loss ((2)).__ Therefore, this question should be rephrased to:\n",
    "\n",
    "What is your opinion of dancing on Sundays? (a) Should be allowed (b) Should be forbidden.\n",
    "\n",
    "## 5.4.2 Who and How to Administer Surveys\n",
    "\n",
    "__For Who__:\n",
    "Some demographic groups are more likely to complete surveys than others. To fix this, you need to apply _weights_ to a given demographic so that it more aligns with the actual demographics of the population. Example of this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baltimore</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Weights (Balt/Sample)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African-American</th>\n",
       "      <td>63</td>\n",
       "      <td>50</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Baltimore  Sample  Weights (Balt/Sample)\n",
       "African-American         63      50                   1.26\n",
       "White                    28      40                   0.70\n",
       "Asian                     2       1                   2.00"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Baltimore': [63, 28, 2], \n",
    "                       'Sample': [50, 40, 1],\n",
    "                       'Weights (Balt/Sample)': [1.26,.7, 2]},\n",
    "                 index=['African-American', 'White', 'Asian'])\n",
    "\n",
    "df\n",
    "#Here, weights are used to adjust sample to represent actual population\n",
    "# weighted adjustments would be used for other surveying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question (and Response) Order:__\n",
    "- Questions later in a survey are more likely to be skipped\n",
    "- Questions in the beginning may affect answers later on (e.g., asking women their gender before giving them a math test led to lower scores on the test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.3 The Pros and Cons of Self-Reporting\n",
    "\n",
    "__Pros:__\n",
    "- It's cheap\n",
    "- It's easy to acquire\n",
    "- For anonymous surveying, it's a good way to get information on topics that are too private, rare, or complex.\n",
    "\n",
    "__Cons:__\n",
    "- Self-reporting is a VERY weak indicator of what people _actually_ do.\n",
    "- People are more likely to self-report when they have strong feelings about something, thus biasing the data to a lack of middle-of-the-road people (especially when it comes to customer reviews).\n",
    "- People are biased in their fear of reporting downsides, even if it is anonymous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drill\n",
    "Is a survey the best option for each of the following questions?\n",
    "\n",
    "1. Popularity of various Uber drivers. \n",
    "\n",
    "__ANSWER:__ Yes. People can report their Uber experiences in real-time when the information is fresh in their mind. With that, the use of technology allows for a widespread application across demographics (all demos nowadays have a phone). There is also no sensitive information being divulged in this kind of survey.  These answers will need to be weighted due to socioeconomic demographics, as richer people will tend to use Uber more.\n",
    "\n",
    "2. A grocery store wants to know how much people like strawberry, chocolate, and vanilla ice cream. \n",
    "\n",
    "__ANSWER:__ No. This question is far too broad (premium vanilla is far better than bottom-shelf ice cream). If the survey were coupled with taste-testing, then people may provide more accurate survey answers. However, this hypothetical is not part of the prompt, and so the answer remains no.\n",
    "\n",
    "3. Trojan wants to know whether their condoms are more popular among straight people or gay people. \n",
    "\n",
    "__ANSWER:__ No. Condoms are a sensitive topic, and sexual orientation is another piece of sensitive information. There will be significant biases in this survey. This information will have to be obtained another way.\n",
    "\n",
    "4. Recently, rental rates for HappyCars have increased by 30%, which is quite unusual. You want to find out why this is happening and how to keep the increase going. \n",
    "\n",
    "__ANSWER:__ No. There is no target demographic, except for maybe HappyCars executives. Their answers may not be the entire picture, as they will be unlikely to divulge their business decisions. Also, any market information that might explain this rate increase can be obtained without the need for surveying.\n",
    "\n",
    "5. Assessing the success rate of a new advertising campaign. \n",
    "\n",
    "__ANSWER:__ Yes. You would be able to measure people's reactions and identify what exactly they liked about the campaign that presumably contibuted to the campaign's success. You can also assess demographic differences in terms of their respective receptions of the campaign.\n",
    "\n",
    "6. Measuring the effect of nurse/patient ratio on patient recovery rates in hospital wards.\n",
    "\n",
    "__ANSWER:__ No. First of all, there is a lot of sensitive information to overcome. Second, there may be biases in their responses depending on how the patients are faring in their respective conditions. Finally, the information sought can be obtained outside individual surveying - ward records would have that kind of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Advanced Experimentation\n",
    "\n",
    "This secion involves additional statistical analysis methods.\n",
    "\n",
    "## 5.5.1 Parametric Distribution Comparison Tests\n",
    "\n",
    "__What is \"Parametric\"?:__ A test that uses known parameters, such as mean and standard deviation. Such parameters are really only helpful if the variable follows a certain distribution (normal, binomial, poisson, etc.).\n",
    "\n",
    "Sample Parametric Test Analysis (regarding roller coaster data between three different materials_ below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, create and load some coaster height data.\n",
    "\n",
    "coaster_heights = pd.DataFrame()\n",
    "\n",
    "steel_heights = [\n",
    "    18.5, 14, 30.2, 25.2024, 15, 16, 13.5, 30, 20, 17, 13.716, 8.5, 16.1, 18,\n",
    "    41, 30.3, 32.004, 28.004, 30.48, 34\n",
    "    ]\n",
    "\n",
    "wood_heights = [\n",
    "    38.70, 46, 27.8, 43.52, 33.77, 29.26, 16.764, 45, 48.1, 16.764, 24.384,\n",
    "    24.5, 40, 35.96, 22.24, 21.33, 27.73, 23.46, 21.64, 30.12\n",
    "    ]\n",
    "\n",
    "plastic_heights = [\n",
    "    9, 8.2, 12, 21, 6.3, 11.7, 19.44, 4.75, 13, 18, 15.5, 15.6, 10, 11.77, 29,\n",
    "    5, 3.2, 14.75, 18.2, 17.7\n",
    "    ]\n",
    "\n",
    "coaster_heights['Steel'] = steel_heights\n",
    "coaster_heights['Wood'] = wood_heights\n",
    "coaster_heights['Plastic'] = plastic_heights\n",
    "\n",
    "# Visualize our data.\n",
    "coaster_heights.hist(sharey=True,sharex=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While plastic is a little skewed, we can say that they are kind of normally distributed (especially given such a small size of data).\n",
    "\n",
    "Now let's do anlaysis: __when comparing three unrelated groups of data, the appropriate parametric coice is a one-way Analysis of Variance (ANOVA).__\n",
    "\n",
    "\\begin{equation}\n",
    "F=\\frac{{n\\sum(\\bar{Y}_j-\\bar{Y})^2}/{(a-1)}}{{\\sum\\sum(Y_{ij}-\\bar{Y}_j)^2}/{(N-a)}}\n",
    "\\end{equation}\n",
    "\n",
    "*n* is the number of datapoints in each group.  \n",
    "$\\bar{Y}_j$ is the mean for the *j*th group.  \n",
    "$\\bar{Y}$ is the overall mean.  \n",
    "$Y_{ij}$ is the *i*th value of *j*th group.  \n",
    "*a* is the number of groups.  \n",
    "*N* is the total number of datapoints.  \n",
    "\n",
    "ANOVA compares (how much every individual group mean differs from the overall mean) to (how much the observations within each group differ from the mean of that individual group). This results in \"_the probability of getting a difference that large in the sample if there were no meaningful difference between the groups in the population._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steel      22.57532\n",
      "Wood       30.85210\n",
      "Plastic    13.20550\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.515815855142634\n",
      "1.0927449918229045e-07\n"
     ]
    }
   ],
   "source": [
    "# Printing the means for each group.\n",
    "print(coaster_heights.mean())\n",
    "\n",
    "coaster_heights.boxplot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "F, p = stats.f_oneway(\n",
    "    coaster_heights['Steel'],\n",
    "    coaster_heights['Wood'],\n",
    "    coaster_heights['Plastic'])\n",
    "\n",
    "# The F statistic.\n",
    "print(F)\n",
    "\n",
    "# The probability. A p < .05 would lead us to believe the group means were\n",
    "# not all similar in the population.\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-2.785911485728798, pvalue=0.008281657085474181)\n",
      "Ttest_indResult(statistic=3.8446067874474887, pvalue=0.00044650642104150624)\n",
      "Ttest_indResult(statistic=-6.683437837231596, pvalue=6.614422528990009e-08)\n"
     ]
    }
   ],
   "source": [
    "# To confirm if plastic is indeed significantly different, \n",
    "# we will compare all t-tests between these three groups.\n",
    "\n",
    "    print(stats.ttest_ind(coaster_heights['Steel'], coaster_heights['Wood']))\n",
    "    print(stats.ttest_ind(coaster_heights['Steel'], coaster_heights['Plastic']))\n",
    "    print(stats.ttest_ind(coaster_heights['Plastic'], coaster_heights['Wood']))\n",
    "\n",
    "# The t-test shows that in fact, all three groups are statistically different\n",
    "# (i.e., p value is < 0.05 for all t-tests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2 Normality Testing and Multiple Testing Correction\n",
    "\n",
    "A \"__Shapiro-Wilk Test Statistic__, _W_,\" measures the summed difference between what would be expected in a perfectly normally-distributed sample of the size of your actual sample and the actual values. Because it's a ratio, values close to 1 indicate distribution is similar to a normal distribution.\n",
    "\n",
    "\\begin{align} \n",
    "W=\\frac { (\\sum_{i=1}^n a_ix_{(i)} )^2}{ \\sum_{i=1}^n (x_i-\\bar{x})^2}\n",
    "\\end{align}\n",
    "\n",
    "$x_{(i)}$ is the *i*th smallest number in the sample.  \n",
    "$a_i$ is a constant determined by the sample size and acts as a scaling value.  \n",
    "$\\bar{x}$ is the sample mean.  \n",
    "$x_i$ is the *i*th observation in the sample.\n",
    "\n",
    "These kinds of tests are sensitive to sample size. The smaller the sample, the more prone it is to deviations from normality.\n",
    "\n",
    "---\n",
    "\n",
    "Separately, there is also \"__Tukey's Honest Significant Differences (HSD) Test__, _Q_,\" which identifies an estimated _overall_ variability between multiple groups. Think of it as an estimate from multiple t-tests. This is good if you're looking for a single p-value to work from to determine if your test, _overall_, is significant.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Q=\\frac{M_i-M_j}{\\sqrt{MSE/n}}\n",
    "\\end{equation}\n",
    "\n",
    "Where $MSE={\\sum\\sum(Y_{ij}-\\bar{Y}_j)^2}/{(N-a)}$ from the denominator of the F-test above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multiple Comparison of Means - Tukey HSD,FWER=0.05</caption>\n",
       "<tr>\n",
       "  <th>group1</th>  <th>group2</th> <th>meandiff</th>  <th>lower</th>   <th>upper</th>  <th>reject</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Plastic</td>  <td>Steel</td>  <td>9.3698</td>  <td>2.8923</td>  <td>15.8474</td>  <td>True</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Plastic</td>  <td>Wood</td>   <td>17.6466</td> <td>11.1691</td> <td>24.1241</td>  <td>True</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>Steel</td>   <td>Wood</td>   <td>8.2768</td>  <td>1.7992</td>  <td>14.7543</td>  <td>True</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample advanced analyses\n",
    "\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "heights=np.asarray(\n",
    "    coaster_heights['Steel'].tolist() +\n",
    "    coaster_heights['Wood'].tolist() +\n",
    "    coaster_heights['Plastic'].tolist())\n",
    "\n",
    "materials = np.array(['Steel', 'Wood','Plastic'])\n",
    "materials = np.repeat(materials, 20)\n",
    "\n",
    "tukey = pairwise_tukeyhsd(endog=heights,      # Data\n",
    "                          groups=materials,   # Groups\n",
    "                          alpha=0.05)         # Significance level\n",
    "\n",
    "tukey.summary()  # Gives you a nice \"reject\" or \"don't reject\" conclusion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.3 Non-Parametric Distribution Comparison Tests\n",
    "\n",
    "If data is non-normal, and if statistics results assume normality (linear regression, t-tests, ANOVA, and other correlative analyses), parametric tests will yield incorrect results.\n",
    "\n",
    "Non-parametric tests are less powerful but are nonetheless used for these non-normal data scenarios. They convert the variable values into rankings, and statistical tests are performed on those ranks. \n",
    "\n",
    "One example of non-parametric test is the \"__Kruskal-Wallace Test__, _H_\". This test performs ANOVA logic (see in previous section _supra_). This test uses ranks from 1 (highest value) to N (lowest value). The statistic value _H_ represents the same thing as ANOVA, but relative to variance in _ranks_ within each group instead of variance from the mean within each group. The higher the value of _H_, the more likely that one of the groups differs meaningfully from the other groups within the population.\n",
    "\n",
    "\\begin{equation}\n",
    "H=(N-1)\\frac{\\sum_{i=1}^gn_i(\\bar{r}_i-\\bar{r})^2}{\\sum_{i=1}^g\\sum_{j=1}^{n_i}(r_{ij}-\\bar{r})^2}\n",
    "\\end{equation}\n",
    "\n",
    "*H* is the test statistic.  \n",
    "$n_i$ is the number of datapoints in group *i*.   \n",
    "$r_{ij}$ is the rank of datapoint *j* in group *i* when ranked among all datapoints in all groups.  \n",
    "*N* is the total number of datapoints.  \n",
    "$\\bar{r}_i$ is the average rank of all datapoints in group *i*.  \n",
    "$\\bar{r}$ is the average of all the ranks, and can be calculated as $\\frac12(N+1)$.  \n",
    "\n",
    "\n",
    "Example of the Kruskal-Wallce test is found below, using roller coaster heights between different regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, create and load some coaster height data.\n",
    "roller_coasters = pd.DataFrame()\n",
    "\n",
    "europe_heights = [\n",
    "    13.5, 36, 94, 6.1, 22, 35.5, 35.5, 8, 8, 10, 8.5, 5, 30, 31, 4.5, 15, 36,\n",
    "    20, 10, 30.4\n",
    "    ]\n",
    "latin_america_heights = [\n",
    "    33.3, 35.5, 30, 32, 8.5, 60, 8, 33, 33.1, 13, 8, 22.9, 3.3, 11, 11, 19.5,\n",
    "    35.1, 19.5, 4, 35.5\n",
    "    ]\n",
    "north_america_heights = [\n",
    "    66.4, 23.7, 54.5, 22.2, 20, 1.8, 3.9, 35.3, 45.2, 51.2, 43.5, 24.3, 74.6,\n",
    "    14.3, 16.1, 4.8, 38.9, 30.8, 10, 20\n",
    "    ]\n",
    "\n",
    "roller_coasters['Europe'] = europe_heights\n",
    "roller_coasters['LatinAmerica'] = latin_america_heights\n",
    "roller_coasters['NorthAmerica'] = north_america_heights\n",
    "\n",
    "# Visualize our data.\n",
    "roller_coasters.hist(sharey=True, sharex=True)\n",
    "plt.show()\n",
    "# DATA SHOWN IS NON-NORMAL! :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranks_Europe          27.8\n",
      "Ranks_LatinAmerica    28.9\n",
      "Ranks_NorthAmerica    34.8\n",
      "dtype: float64\n",
      "30.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=1.8600661901716153, pvalue=0.39454065279877265)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kruskal-Wallace Test\n",
    "\n",
    "# Join all ratings together into a list, then ranking them.\n",
    "ranks = stats.rankdata(\n",
    "    roller_coasters['Europe'].tolist() +\n",
    "    roller_coasters['LatinAmerica'].tolist() +\n",
    "    roller_coasters['NorthAmerica'].tolist())\n",
    "\n",
    "# Add the new ranked variables to the data frame.\n",
    "roller_coasters['Ranks_Europe'] = ranks[0:20]\n",
    "roller_coasters['Ranks_LatinAmerica'] = ranks[20:40]\n",
    "roller_coasters['Ranks_NorthAmerica'] = ranks[40:60]\n",
    "\n",
    "# Average rank per group.\n",
    "groups = ['Ranks_Europe', 'Ranks_LatinAmerica', 'Ranks_NorthAmerica']\n",
    "print(roller_coasters[groups].apply(np.mean))\n",
    "\n",
    "# Overall average rank.\n",
    "print(np.mean(ranks))\n",
    "\n",
    "# Print the test statistic followed by the probability of getting this result\n",
    "# if the groups were not different.\n",
    "stats.kruskal(\n",
    "    roller_coasters['Europe'],\n",
    "    roller_coasters['LatinAmerica'],\n",
    "    roller_coasters['NorthAmerica'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _H_ score shows that there is a 39.5% chance that the differences are due to noise and not systematic differences between the data groups. No signifcance here - cannot reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
