{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Algorithms and Data Structures\n",
    "\n",
    "## 5.1.1 Basic Data Structures\n",
    "\n",
    "1. __The List__ <br>\n",
    "    a. Ordered sequence of items with an incrementing index (which starts at 0). <br>\n",
    "    b. ___Good because___ we can call any element by simply calling its index. This also makes adding things to the end of a list easy. <br>\n",
    "    c. ___Bad because___ removing something from a list messes up the entire index, because the list _must_ start at 0.\n",
    "    \n",
    "    \n",
    "2. __Linked List__ <br>\n",
    "    a. Maintained order of items _but_ no index - each entry is \"linked\" to the next item. <br>\n",
    "    b. ___Good for___ deletion of data but ___bad for___ accessing items of data.\n",
    "    \n",
    "    \n",
    "3. __Queue__ (\"FIFO\" organization) <br>\n",
    "    a. Specialized linked list where items _can only come in one end and out the other._ (i.e., consisting of \"enqueing items\" and \"dequeing items.\") <br>\n",
    "    b. Huge drawbacks with accessing and searching, as you have to go through the queue sequentially to see if an element is present (resulting in O(n) efficiency).\n",
    "\n",
    "\n",
    "4. __Stack__ (\"LIFO\" organization) <br>\n",
    "    a. Similar drawbacks to the queue, but like the queue, it is useful in certain circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Sorting Items\n",
    "\n",
    "These are algorithms that have different methods to sort a list of items.\n",
    "\n",
    "1. __Insertion Sort__ <br>\n",
    "    a. Take an item, move it to the sorted spot and push the associated number of items up or down in a list. <br>\n",
    "    b. O(n^2) timing, where n = length of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random \n",
    "\n",
    "random.seed(a=100)\n",
    "\n",
    "short_list = list(random.sample(range(1000000), 10))\n",
    "long_list = list(random.sample(range(1000000), 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sort(input_list):\n",
    "    # Copy the input to a new list so we don't modify the original.\n",
    "    new_list = input_list\n",
    "    \n",
    "    # Iterate through the list.\n",
    "    for i in range(len(new_list)):\n",
    "        # Assign place to a new variable.\n",
    "        j = i\n",
    "        \n",
    "        # Move through the list as long as the previous position is larger\n",
    "        # than the current element of list.\n",
    "        while j > 0 and new_list[j - 1] > new_list[j]:\n",
    "            \n",
    "            # Swap places.\n",
    "            new_list[j - 1], new_list[j] = new_list[j], new_list[j - 1]\n",
    "            \n",
    "            # Reduce j by one.\n",
    "            j = j - 1\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n",
      "[152745, 183236, 366725, 412125, 477025, 481850, 739784, 767514, 808225, 997948]\n",
      "\n",
      "--- 8.67147421836853 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# checking how long an insertion sort takes\n",
    "# Start the timer.\n",
    "start_time = time.time()\n",
    "\n",
    "# Run our insertion sort.\n",
    "insert_sort(short_list)\n",
    "\n",
    "# Print time to show runtime.\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(insert_sort(short_list))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "insert_sort(long_list)\n",
    "\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Merge Sort__ <br>\n",
    "    a. Divides and conquers the list by breaking up the list into parts, sorting those parts, and re-combining the list. <br>\n",
    "    b. Much faster: O(n(log(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our merge function takes two ordered lists and merges them together into one ordered list\n",
    "\n",
    "def merge(a, b):\n",
    "    # Check for empty list.\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return a or b\n",
    "    \n",
    "    # Start with an empty result.\n",
    "    result = []\n",
    "    # Track two indexes.\n",
    "    i, j = 0, 0\n",
    "    # Set a while condition to ensure we iterate only for the length of our two lists.\n",
    "    while (len(result) < len(a) + len(b)):\n",
    "        # If a's next element is lower append that element to our result.\n",
    "        if a[i] < b[j]:\n",
    "            result.append(a[i])\n",
    "            i += 1\n",
    "        # Otherwise append b's next element.\n",
    "        else:\n",
    "            result.append(b[j])\n",
    "            j += 1\n",
    "        # When one list is empty just append everything from the other list and stop.\n",
    "        if i == len(a) or j == len(b):\n",
    "            result.extend(a[i:] or b[j:])\n",
    "            break \n",
    "\n",
    "    return result\n",
    "\n",
    "def merge_sort(lst):\n",
    "    if len(lst) < 2:\n",
    "        return lst\n",
    "\n",
    "    mid = int(len(lst) / 2)\n",
    "    a = merge_sort(lst[:mid])\n",
    "    b = merge_sort(lst[mid:])\n",
    "\n",
    "    return merge(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n",
      "[152745, 183236, 366725, 412125, 477025, 481850, 739784, 767514, 808225, 997948]\n",
      "\n",
      "--- 0.07773399353027344 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Test on short list.\n",
    "# Start timer.\n",
    "start_time = time.time()\n",
    "\n",
    "# Run our insertion sort.\n",
    "merge_sort(short_list)\n",
    "\n",
    "# Print time to show runtime.\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(merge_sort(short_list))\n",
    "# Test on long list.\n",
    "start_time = time.time()\n",
    "\n",
    "merge_sort(long_list)\n",
    "\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# compare the above to python cheating way: \"sorted(list_to_sort)\"\n",
    "\n",
    "# Start Timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Sort the default list. Note that .sort() will sort in place, which would alter default_list.\n",
    "sorted(long_list)\n",
    "\n",
    "# Print time to show runtime\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 Trees\n",
    "\n",
    "Another algorithm to be explored. Top down, beginning with the _root_, from the _parent_ root node, there are _children_ nodes. If the node does not have any children, it is a terminal _leaf_ node.\n",
    "\n",
    "The benefit to a tree is that, unlike lists, they are much more flexible. There can be two, three, or more children per node.\n",
    "\n",
    "__When to use Tree Algorithms:__\n",
    "- Hierarchical data (departments in an agency, models and their parts, etc.)\n",
    "\n",
    "A subset of trees is a __heap.__ This is a tree where a parent is _always_ a higher value than its children.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: [a0]\n",
      "Node: [b0]\n",
      "Node: [b1]\n",
      "Node: [b2]\n",
      "Node: [c0]\n",
      "Node: [c1]\n",
      "Node: [d0]\n",
      "\n",
      "Node: [a0]\n",
      "Node: [b0]\n",
      "Node: [c0]\n",
      "Node: [d0]\n",
      "Node: [c1]\n",
      "Node: [b1]\n",
      "Node: [b2]\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE OF A TREE\n",
    "\n",
    "\"\"\"\n",
    "GOAL: Implement a binary tree, which is filled with 15 pieces of random data. \n",
    "Your job is to then write a program to traverse the tree using a breadth first traversal. \n",
    "If you want additional practice, try other forms of traversal.\n",
    "\n",
    "recipe_simple_tree_traversal.py\n",
    "\n",
    "Simple breadth-first and depth-first tree traversal for any tree.\n",
    "\n",
    "The recipe is contained within the first two functions. The rest is\n",
    "a test bed for demonstration.\n",
    "\"\"\"\n",
    "__author__ = \"Jack Trainor\"\n",
    "__date__ = \"2015-12-16\"\n",
    "\n",
    "import sys\n",
    "    \n",
    "def get_breadth_first_nodes(root):\n",
    "    nodes = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        cur_node = stack[0]\n",
    "        stack = stack[1:]\n",
    "        nodes.append(cur_node)\n",
    "        for child in cur_node.get_children():\n",
    "            stack.append(child)\n",
    "    return nodes\n",
    "\n",
    "def get_depth_first_nodes(root):\n",
    "    nodes = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        cur_node = stack[0]\n",
    "        stack = stack[1:]\n",
    "        nodes.append(cur_node)        \n",
    "        for child in cur_node.get_rev_children():\n",
    "            stack.insert(0, child)\n",
    "    return nodes\n",
    "\n",
    "########################################################################\n",
    "class Node(object):\n",
    "    def __init__(self, id_):\n",
    "        self.id = id_\n",
    "        self.children = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Node: [%s]\" % self.id\n",
    "    \n",
    "    def add_child(self, node):\n",
    "        self.children.append(node) \n",
    "    \n",
    "    def get_children(self):\n",
    "        return self.children         \n",
    "    \n",
    "    def get_rev_children(self):\n",
    "        children = self.children[:]\n",
    "        children.reverse()\n",
    "        return children         \n",
    "\n",
    "########################################################################\n",
    "def println(text):\n",
    "    sys.stdout.write(text + \"\\n\")\n",
    "    \n",
    "def make_test_tree():\n",
    "    a0 = Node(\"a0\")\n",
    "    b0 = Node(\"b0\")      \n",
    "    b1 = Node(\"b1\")      \n",
    "    b2 = Node(\"b2\")      \n",
    "    c0 = Node(\"c0\")      \n",
    "    c1 = Node(\"c1\")  \n",
    "    d0 = Node(\"d0\")   \n",
    "    \n",
    "    a0.add_child(b0) \n",
    "    a0.add_child(b1) \n",
    "    a0.add_child(b2)\n",
    "    \n",
    "    b0.add_child(c0) \n",
    "    b0.add_child(c1) \n",
    "    \n",
    "    c0.add_child(d0)\n",
    "    \n",
    "    return a0                  \n",
    "\n",
    "def test_breadth_first_nodes():\n",
    "    root = make_test_tree()\n",
    "    node_list = get_breadth_first_nodes(root)\n",
    "    for node in node_list:\n",
    "        println(str(node))\n",
    "\n",
    "def test_depth_first_nodes():\n",
    "    root = make_test_tree()\n",
    "    node_list = get_depth_first_nodes(root)\n",
    "    for node in node_list:\n",
    "        println(str(node))\n",
    "\n",
    "########################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    test_breadth_first_nodes()\n",
    "    println(\"\")\n",
    "    test_depth_first_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: [a0]\n",
      "Child is   Node: [b0]\n",
      "Child is   Node: [b1]\n",
      "Child is   Node: [b2]\n",
      "Node: [b0]\n",
      "Child is   Node: [c0]\n",
      "Child is   Node: [c1]\n",
      "Node: [b1]\n",
      "Node: [b2]\n",
      "Node: [c0]\n",
      "Child is   Node: [d0]\n",
      "Node: [c1]\n",
      "Node: [d0]\n",
      "Node: [a0]\n",
      "Node: [b0]\n",
      "Node: [b1]\n",
      "Node: [b2]\n",
      "Node: [c0]\n",
      "Node: [c1]\n",
      "Node: [d0]\n"
     ]
    }
   ],
   "source": [
    "def get_breadth_first_nodes(root):\n",
    "    nodes = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        cur_node = stack[0]\n",
    "        print(cur_node)\n",
    "        stack = stack[1:]\n",
    "        nodes.append(cur_node)\n",
    "        for child in cur_node.get_children():\n",
    "            print('Child is  ', child)\n",
    "            stack.append(child)\n",
    "    return nodes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_breadth_first_nodes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Scraping\n",
    "\n",
    "__Scraping__ is the act of extracting data from a website and saving it to a file (or database). A __scraper__ is the function that does that.\n",
    "\n",
    "(In practicality, most employers don't care about scraping because they have _too much data_).\n",
    "\n",
    "## 5.2.1 What is scraping?\n",
    "\n",
    "Three step process:\n",
    "1. Access webpages\n",
    "    - This is through python, not through a web browser\n",
    "2. Locate the desired information/data from those webpages\n",
    "    - This is called __parsing__ the data - pulling out specific information you want.\n",
    "3. Save that information in a different location\n",
    "    - save to local terminal or dump into SQL database\n",
    "    \n",
    "__Scraping Python Libraries:__\n",
    "- _Requests_ (for web pages)\n",
    "- _BeautifulSoup_ or _lxml_ (for paring)\n",
    "- _JSON_ or _pymysql_ (for storage)\n",
    "\n",
    "## 5.2.2 Scrapy\n",
    "\n",
    "[Scrapy](https://doc.scrapy.org/en/latest/topics/api.html) is a widely used scraper. The user provides one or more starting URLs, and the scraper sends a request to the server for the information at that URL. The server provides the information requested (or an error code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: UserWarning: You do not have a working installation of the service_identity module: 'cannot import name 'verify_ip_address' from 'service_identity.pyopenssl' (C:\\Anaconda\\lib\\site-packages\\service_identity\\pyopenssl.py)'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n",
      "2019-05-01 18:55:40 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n",
      "2019-05-01 18:55:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1a  20 Nov 2018), cryptography 2.4.2, Platform Windows-10-10.0.17763-SP0\n",
      "2019-05-01 18:55:40 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-05-01 18:55:41 [scrapy.extensions.telnet] INFO: Telnet Password: 65cd151eb269e42c\n",
      "2019-05-01 18:55:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-01 18:55:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-01 18:55:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-01 18:55:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-01 18:55:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-01 18:55:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-01 18:55:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2019-05-01 18:55:42 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://everydaysexism.com/> from <GET http://www.everydaysexism.com>\n",
      "2019-05-01 18:55:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://everydaysexism.com/> (referer: None)\n",
      "2019-05-01 18:55:42 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-01 18:55:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 438,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 11679,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 1, 22, 55, 42, 855371),\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 8,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2019, 5, 1, 22, 55, 41, 826587)}\n",
      "2019-05-01 18:55:42 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# designing a basic scraper with scrapy\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.everydaysexism.com', # sample URL here\n",
    "    ]\n",
    "\n",
    "    # What to do with the URL.  Here, we tell it to download all the code and save\n",
    "    # it to the mainpage.html file\n",
    "    def parse(self, response):\n",
    "        with open('mainpage.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "\n",
    "\n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUICK NOTE:\n",
    "\n",
    "The kernel for scrapy needs to be restarted ___each time you run it.___ If you don't restart the kernel, you'll get a \"ReactorNotRestartable\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-df5af79b660a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Start the crawler with our spider.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mESSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Success!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scrapy\\crawler.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, stop_after_crawl)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shutdown'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1272\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \"\"\"\n\u001b[0;32m   1250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m         \u001b[0mReactorBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.everydaysexism.com',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.********\n",
    "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading what was scraped into the json file - placing it in a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "firstpage = pd.read_json('firstpage.json', orient='records')\n",
    "print(firstpage.shape)\n",
    "print(firstpage.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis only scraped a single webpage. If you want to scrape _all of the pages_ of a website, scrapy will have to call itself via a recursive algorithm. Scrapy will identify what pages are connected to the webpage being scraped, and it will scrape those pages too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 Breaking the Website\n",
    "\n",
    "The speed at which scrapers access a page may cause the website to crash. In fact, you may manually adjust the scraper parameter to introduce _delays_ in between calls to the web server. Other ways to minimize crashing the site include:\n",
    "\n",
    "- _Autothrottling_, which dynamically sets the delay based on how quickly the server responds to scrapy's requests.\n",
    "- _Caching_, where scrapy will use a cached copy of the website (found on your terminal) instead of calling the exact page from the website _again_.\n",
    "\n",
    "__Robots__\n",
    "for every website, there is a \"robots\" file that tells scrapy's which portions of the sites are up for grabs and which are off-limits. For example, Amazon's is: https://amazon.com/robots.txt. If you scrape something that is forbidden: scrapy's traceback will indicate `[scrapy] DEBUG: Forbidden by robots.txt:<GET http://website.com/login>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4 Application Programming Interfaces (APIs)\n",
    "\n",
    "Large companies use an API in lieu of their actual websites; in other words, you access the data from the API, not from manually scraping the website. This makes it easier to obtain the website's information, and at the same time it doesn't clog up the website with scraping.\n",
    "\n",
    "__Accessing an API__\n",
    "Well, you need an _API key_ or _API token_.\n",
    "\n",
    "__The Basics of an API__\n",
    "Users have __Access__ via a key. There they make __requests__ to the API (the data you want with a call to the API), and the API provides a __response__, which is the return of data to the API (usually in .json format).\n",
    "\n",
    "#### Example API - Wikipedia API\n",
    "\n",
    "Sometimes, you still need a scrapy even when working with an API. This is because the API will only provide a limited number of responses (which is intentional to prevent server overloading). A scrapy would go ahead and make multiple calls _to the API._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4c2dc776e969>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Starting the crawler with our spider.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWikiSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'First 100 links extracted!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scrapy\\crawler.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, stop_after_crawl)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shutdown'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1272\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \"\"\"\n\u001b[0;32m   1250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m         \u001b[0mReactorBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
