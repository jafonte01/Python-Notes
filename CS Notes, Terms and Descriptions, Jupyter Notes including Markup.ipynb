{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTER SCIENCE NOTES\n",
    "\n",
    "These are the things you should know when it comes to computer science.\n",
    "\n",
    "- Memory\n",
    "- Big O and related notations\n",
    "- Some OS command line stuff\n",
    "\n",
    "__NOTE!!!!!!!!__ Section not completed - need to complete.\n",
    "\n",
    "***\n",
    "\n",
    "## Memory\n",
    "\n",
    "One bit = identifying number, either in binary (0b------) or hexadecimal (0x-------)\n",
    "\n",
    "Every bit is a single digit, so \"12\" in hexadecimal is \"C\" (0 through 9, then A for \"10\", B for \"11\" and so on)\n",
    "\n",
    "_Calculating binary to decimal:_\n",
    "\n",
    "binary = 10010\n",
    "\n",
    "do: \"0 x 2^0 + 1 x 2^1 + 0 x 2^2 + 0 x 2^3 + 1 x 2^4\" = 18\n",
    "\n",
    "[Calculating decimal to hexadecimal](https://www.permadi.com/tutorial/numDecToHex/)\n",
    "\n",
    "[Calculating hexadecimal to decimal](https://www.permadi.com/tutorial/numHexToDec/index.html)\n",
    "\n",
    "8 bits = 1 byte\n",
    "\n",
    "1 byte = \"N\" = 2$^{8N}$ - 1 = 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Programming\n",
    "\n",
    "Four main types, in order from least abstract to most abstract (but not necessarily easiest to hardest!):\n",
    "\n",
    "1. __Procedural Programming__\n",
    "\n",
    "\n",
    "2. __Object-Oriented Programming__\n",
    "\n",
    "\n",
    "3. __Functional Programming__\n",
    "\n",
    "\n",
    "4. __Quantum Programming__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# JUPYTER NOTES\n",
    "\n",
    "These are notes you need for the following things:\n",
    "\n",
    "- Some Hotkeys and saving methods in Jupyter\n",
    "- Any Jupyter Errors you may come across \n",
    "- Jupyter Markup Language\n",
    "\n",
    "__NOTE!!!!!!!!__ Overall, section not completed - need to complete.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Things to Know in Jupyter\n",
    "\n",
    "1. __HOTKEYS__ <br>\n",
    "\n",
    "The following are hotkeys used within Jupyter for cell manipulation. Instead of compiling an entire page, one can run a single block (_cell_) of code at a time.\n",
    "<br><br>\n",
    "__To use a hotkey, one must be in _command mode___. If the cell is green on the left-hand side of the cell, it is not in command mode, which is in blue. To get into command mode, press either the `Esc` key or mouse click on the white space between the colored line and the cell. Once the line turns blue and you are in command mode, you can use any of the following hotkeys to manipulate Jupyter cells. A complete list of hotkeys can be found in the _Command Palette_ at the top of the Jupyter toolbar.\n",
    "- To delete a cell..................................... `d` key twice <br><br>\n",
    "- To undo cell deletion................................ `z` key<br><br>\n",
    "- To create a new cell above........................... `a` key (`b` key for new cell below)<br><br>\n",
    "- To convert a cell to HTML markup..................... `m` key (`y` key for coding cell)<br><br>\n",
    "- To cut and paste cells............................... `x` and `v` keys, respectively (`c` for copy)<br><br>\n",
    "- To access command mode of multiple cells at one time. `Shift` + up or down arrow keys.<br>\n",
    "    - This only allows you to access _consecutive_ multiple cells. You would have to change the order of the cells first before moving non-consecutive cells. This can be down by clicking the up or down arrows on the Jupyter toolbar (no hotkeys for those :/)<br><br>\n",
    "- To compile and run a cell............................. `Shift + Enter` keys (will move to next cell; do `Ctrl + Enter` if you want to stay on same cell)<br><br>\n",
    "- To toggle (hide/show) cell output..................... `O` key, or double click on the output with mouse.\n",
    "<br><br>\n",
    "***\n",
    "2. __Adjusting Size of Output__ (e.g. displaying more columns for pandas dataframe than is shown by default)\n",
    "\n",
    "Section not completed. Need to complete.\n",
    "***\n",
    "3. __Saving Jupyter Notebook in Other Formats (slideshow, pdf, etc.)__\n",
    "\n",
    "Section not completed. Need to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Jupyter-Wide Errors\n",
    "\n",
    "`_xsrf argument missing from  POST.` - Save error\n",
    "\n",
    "__Answer:__ This occurs when the kernel died and reset, but your notebook is still up and you are working on the notebook from the same dead kernel. Open a _separate notebook_ from the port, which will \"re-activate\" the kernel, and you will be able to save the notebook no problem.\n",
    "\n",
    "***\n",
    "### Python-Specific Syntax Errors\n",
    "\n",
    "`object is not subscriptable` <br>\n",
    "__Answer:__ replace brackets with parentheses\n",
    "\n",
    "`EOF Parsing error` <br>\n",
    "__Answer:__ Missing a parenthesis\n",
    "\n",
    "`Truth-value of a series is ambiguous` <br>\n",
    "__Answer:__ Python refuses to check the truth-value of things with more than one value (lists, tuples, pandas series, etc.). Do a list comprehension where the conditional can be applied to every individual datapoint.\n",
    "\n",
    "`ValueError: Too many values to unpack (or not enough values to unpack)` <br>\n",
    "__Answer:__ You set too many variables but did not give enough values (or the opposite - too many values but not enough variables to assign). This is a common mistake when zipping several variables.\n",
    "\n",
    "`ValueError: Setting an array element with a sequence`<br>\n",
    "__Answer:__ When comparing elements within an array, the lengths of those elements _must be the identical length_. An irregular length, such as np.array(\\[1,2\\], \\[1,4,5,6\\], \\[11\\]) will trigger this error - you would have to make the second element smaller to length of 2 to fix.\n",
    "\n",
    "`SyntaxError: Positional Argument follows keyword argument`<br>\n",
    "__Answer:__ Keyword argument is a variable with default setting identified by equal sign (usually found in function calls, param1=122). This error states that you have (at least) one too many variables not assigned by an equal sign (a \"positional argument\") _after_ the keyword argument, and so python doesn't know what to do with it. For example: `my_funct(x, y, z=111)` --> (calling) `my_func(x=1, y=22, z=44, 132)`. In short, you probably forgot a comma or an equal sign --> check your syntax m8.\n",
    "\n",
    "`EOF when reading a line.` (_input error_) <br>\n",
    "__Answer:__ This shows that the compiler is performing a function before breaking. In other words, it uses up all the input it has then throws an error saying it doesn't know what to do because all the input was used up but the function wants it to keep going.\n",
    "\n",
    "This is often a problem with STDIN and STDOUT manual inputting - the number of inputs requested exceeds the number of inputs assigned. Fix this by not using a loop containing input():\n",
    "\n",
    "```python\n",
    "N = []\n",
    "\n",
    "for i in range(2): # 2 = number of inputs\n",
    "    N.append(list(map(int, input().rstrip().split()))) # inputs are translated as strings in python - gotta change that!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Jupyter Markup Notes (rendered in HTML)\n",
    "\n",
    "__NOTE:__ <br> \n",
    "This section, found in the following cell, is purposely done in a regular scripting/code cell because I don't want to escape every single thing I explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n*****************PARAGRAPH FORMATTING**********************\\n#  Line Break - Type <br> inline \\n                (use <br><br> to separate paragraphs)\\n                \\n#  Horizontal Line Divider - Three asterisks (***) or dashes (---)\\n\\n#  Numbering - done automatically:\\n1. Thingy\\n2. Another Thingy\\n    - (tab, hyphen, space) Subthingy for 2.\\n3. Final Thingy\\n\\n## Use just hyphens for bulleted format\\n- (hyphen, space) sample first bullet\\n- then second bullet\\n    - (tab, hyphen, space) sub-second bullet\\n        - (tab tab) yes you can go multiple subs\\n        \\nSometimes numbering and bullets is finnecky within markup. \\nBe creative with line breaks and horizontal dividers to make it work.\\n\\n************************OTHER FORMATTING***********************\\n\\n# Insertion of image:\\n\\n(From URL):\\n<img src='ImageURL'>\\n\\n(From local file - MUST be located in same folder as notebook!):\\n<img src='file'>\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "****************TEXT FORMATTING********************\n",
    "\n",
    "#  Italics use single underscore (_text_) \n",
    "   or single asterisk (*text*)\n",
    "\n",
    "#  Bolding uses double underscore (__text__)\n",
    "   or double asterisk (**text**)\n",
    "   \n",
    "#  Doing BOTH bolding and italics uses, you guessed it, \n",
    "   three underscores or asterisks (___text___)\n",
    "   \n",
    "#  Placing code within markup requires back apostrophes\n",
    "   (`inputted code, to be outputted as text markup`)\n",
    "\n",
    "#  Subscript (find HTML - section not completed)\n",
    "\n",
    "#  Superscript (find HTML - section not completed)\n",
    "\n",
    "#  In-line hyperlink uses bracketed inline text \n",
    "   followed by the URL in parentheses\n",
    "   ([this is the text that will be in blue as the hyperlink]\n",
    "   (this is where you place the URL)\n",
    "    \n",
    "   e.g.: [wikipedia link here](https://wikipedia.org/John_is_awesome/))\n",
    "    \n",
    "#  Colors (find HTML - section not completed)\n",
    "\n",
    "#  Escape character is forward slash (\\). Can use for any markup formatting\n",
    "\n",
    "example: \\__here you go__ would give you:\n",
    "         italicized \"_here you go_\" because it only escaped one underscore\n",
    "         Therefore, would need \\_\\_here you go__ \n",
    "         for regular \"__here you go\" text\n",
    "'''\n",
    "\n",
    "'''\n",
    "**************HEADER FORMATTING*************\n",
    "Use pound sign (#) for header\n",
    "\n",
    "# Largest Header\n",
    "## Second Hargest header\n",
    "...\n",
    "#### Smallest Header\n",
    "##### Italicized Smallest header \n",
    "(there is a 6x # too but I can't tell the difference between that and 5x)\n",
    "\n",
    "# Centering and moving things over (find HTML - section not completed)\n",
    "'''\n",
    "\n",
    "'''\n",
    "*****************PARAGRAPH FORMATTING**********************\n",
    "#  Line Break - Type <br> inline \n",
    "                (use <br><br> to separate paragraphs)\n",
    "                \n",
    "#  Horizontal Line Divider - Three asterisks (***) or dashes (---)\n",
    "\n",
    "#  Numbering - done automatically:\n",
    "1. Thingy\n",
    "2. Another Thingy\n",
    "    - (tab, hyphen, space) Subthingy for 2.\n",
    "3. Final Thingy\n",
    "\n",
    "## Use just hyphens for bulleted format\n",
    "- (hyphen, space) sample first bullet\n",
    "- then second bullet\n",
    "    - (tab, hyphen, space) sub-second bullet\n",
    "        - (tab tab) yes you can go multiple subs\n",
    "        \n",
    "Sometimes numbering and bullets is finnecky within markup. \n",
    "Be creative with line breaks and horizontal dividers to make it work.\n",
    "\n",
    "************************OTHER FORMATTING***********************\n",
    "\n",
    "# Insertion of image:\n",
    "\n",
    "(From URL):\n",
    "<img src='ImageURL'>\n",
    "\n",
    "(From local file - MUST be located in same folder as notebook!):\n",
    "<img src='file'>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Computer Science Terminology\n",
    "\n",
    "\n",
    "__Difference Between IDE, API, SDK, and other related terms__: [check this link here](https://stackoverflow.com/questions/8772746/difference-between-framework-vs-library-vs-ide-vs-api-vs-sdk-vs-toolkits)\n",
    "\n",
    "***\n",
    "\n",
    "Below are a number of terms in alphabetical order representing computer science concepts, devops methods, programs, programming languages, and descriptions of various acronyms.\n",
    "\n",
    "The format will go as follows: <br>\n",
    "__Term \\[extension of term\\] (abbreviation)__ (company) (_term category_)\n",
    "\n",
    "Companies referred to in the list are found here:\n",
    "- Adobe\n",
    "- Amazon\n",
    "- Apache\n",
    "- Cloudera\n",
    "- RedHat\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #\n",
    "\n",
    "- 5G \n",
    "    - vs. 4G and 3G\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A\n",
    "\n",
    "- Agile \n",
    "    - vs. Scrum\n",
    "- AIX\n",
    "- Ajax\n",
    "- Alteryx\n",
    "- Amazon Web Services (AWS)\n",
    "    - vs. ECS\n",
    "    - vs. EKS\n",
    "    - vs. Lambda\n",
    "    - vs. S3\n",
    "    - vs. Redshift (vs. Openshift)\n",
    "- Angular\n",
    "    - vs. React\n",
    "- Ansible\n",
    "- APM (metric)\n",
    "- Appdynamics\n",
    "- Architecture Guild\n",
    "- Artifact\n",
    "- @risk\n",
    "- \"At the Edge\"\n",
    "- Authentication\n",
    "- AutoML\n",
    "- Avro\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B\n",
    "\n",
    "- Bamboo\n",
    "- Bash\n",
    "\n",
    "\n",
    "- __Batch Processing vs. In-Memory Processing__ (_computer science / big data_)\n",
    "    - Batch processing obtains data in large \"batches\", whereas in-memory puts the memory into RAM and moves/processes it in real-time. In-memory gives near zero latency in the processing, but obviously much much more expensive.\n",
    "    \n",
    "    \n",
    "- Beats (Databeats?)\n",
    "- BERT\n",
    "- Brio\n",
    "- Buffer (program)\n",
    "- Business Intelligence\n",
    "    - vs. Business Logic\n",
    "    - DundasBI\n",
    "    - Oracle BI\n",
    "    - Tibco Spotfire\n",
    "    - Qlik\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C\n",
    "\n",
    "- Canary Analysis/Verification\n",
    "- CAP Theorem\n",
    "\n",
    "\n",
    "- Cassandra (Apache) (_database_)\n",
    "    - NoSQL Database similar to HBase, MongoDB, and Kafka.\n",
    "    \n",
    "    \n",
    "- Central Limit Theorem\n",
    "    - vs. Law of Large Numbers - Check Email\n",
    "- Circle\n",
    "    - “Ci” and “cd”\n",
    "- Cloud Computing (_cloud computing_)\n",
    "    - Various “service” models are found within this wiki article\n",
    "    \n",
    "    \n",
    "- Cloudwatch\n",
    "- Cocoaheads\n",
    "- Collapse OS (may not be on wikipedia)\n",
    "- Command Line\n",
    "- __Commodity \\[Cluster Computing\\]__ (_big data_)\n",
    "    - The process of using multiple computers and other computational components for parallel computing of single processes. The name is taken from the use of \"commodity\" components, which are often cheaper and more standardized than name-brand vendor components (and thus lend themselves better to cluster computing).\n",
    "- Computational time\n",
    "- Conjoint\n",
    "- Consul\n",
    "- Content Delivery Network (CDN)\n",
    "- Continuous Integration (CI)\n",
    "- Crystal Ball\n",
    "- Customer Relationship Management (CRM) System\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D\n",
    "\n",
    "- DataBricks\n",
    "- Dataiku\n",
    "- DC/OS\n",
    "- Decision Problem\n",
    "    - vs. Halting Problem\n",
    "- Dependency\n",
    "- Device Farm\n",
    "- Digital Signal Processing\n",
    "- Distributed Cache\n",
    "- Docbase\n",
    "- DSS Reporting\n",
    "- DTM (Adobe)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E\n",
    "\n",
    "- EC2 (Amazon)\n",
    "- Eclipse Che (Redhat)\n",
    "    - vs. Eclipse Theia\n",
    "- The Elastic Stack (Logstash, Elasticsearch, Kibana)\n",
    "- ElasticSearch\n",
    "- ELK\n",
    "- Emulator\n",
    "- Enterprise Integration Service Oriented Architecture\n",
    "- Entity-Control-Boundary\n",
    "- Ethnos\n",
    "- Event Partitioning\n",
    "- Events-Based Programming/Systems\n",
    "    - Synchronus vs. Asynchronus\n",
    "    - MongoAtlas\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F\n",
    "\n",
    "- Fault Tolerance/Fault-Tolerant Computing\n",
    "- Federation and federated data (security?)\n",
    "- Federations (\"Documentum\" database)\n",
    "- Fedora\n",
    "- Flexicast\n",
    "- Flutter\n",
    "- Flume (Apache)\n",
    "- Fragmenting Data\n",
    "    - vs. \"Sharding\"\n",
    "    \n",
    "    \n",
    "- __Framework__ (_computer science_)\n",
    "    - A set of processes or algorithms that are applied into a programming language; it does not do anything on its own.\n",
    "        - This is akin to a framework being a \"key\": it doesn't do anything unless it is applied to a keyhole.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G\n",
    "\n",
    "- GDPR Flow\n",
    "- Gitlab\n",
    "- Golang\n",
    "\n",
    "\n",
    "- __Google Cloud Platform (GCP)__ (_Cloud Computing_)\n",
    "    - Cloud PaaS that offers virtual machine processing, such as Spark clustering via GCP's DataProc. \n",
    "    - Akin to AWS.\n",
    "    \n",
    "    \n",
    "- GraphQL\n",
    "- GraphX\n",
    "- Gremlin\n",
    "- Groovy\n",
    "- GUI\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H\n",
    "\n",
    "- __H-Base__ (Apache) (_big data_)\n",
    "    - Column-oriented NoSQL database built ontop of Hadoop. Stores data as key-value pairs that allows for random access, which is great for schema-less or completely unstructured databases.\n",
    "\n",
    "\n",
    "- __Hadoop__ (Apache) (_big data_)\n",
    "    - Open Source framework that allows for the distributed processing of large data sets across clusters of commodity computers. The five most common [\"Hadoop Ecosystem\"](https://www.ironsidegroup.com/2015/12/01/hadoop-ecosystkey-components/) tools are:\n",
    "        - H-Base\n",
    "        - HDFS\n",
    "            - Reference to \"Hadoop\" almost always refers to HDFS.\n",
    "        - Hive\n",
    "        - Pig\n",
    "        - Sqoop\n",
    "        \n",
    "        \n",
    "- __Hadoop Distributed File System (HDFS)__ (_big data_)\n",
    "    - A scalable and fault-tolerant file system designed to run on commodity cluster hardware for parallel processing. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system's data. How it works:\n",
    "        - There is a \"master/slave\" setup. The master is a high-end server and the rest are commodity server slaves.\n",
    "        - Each slave stores a piece of the big data file(s) as a \"sequence of blocks\", with each block being a standard unit size.\n",
    "            - For each file in HDFS, you can configure the size of blocks and the number of replications of that file. \n",
    "            \n",
    "            \n",
    "- Heartbeat\n",
    "\n",
    "\n",
    "- __Hive__ (Apache) (_big data_)\n",
    "    - Data processing framework built ontop of HDFS for querying and analysis.\n",
    "        - Very similar to Pig in its purposes, but there are differences between the two (e.g., Pig has procedural oriented language like \"Load 'Some Data'; Dump A\" while Hive has declarative SQL-like language, like \"Select 'Some Data' from A\").\n",
    "        - Pig is often used by programmers and researchers while Hive lends itself more to general use for data report-creating.\n",
    "    - Uses its own SQL language: \"HiveQL\".\n",
    "    \n",
    "    \n",
    "- HSQLDB\n",
    "- HTML\n",
    "    - vs. HTML5\n",
    "- Hyperion\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I\n",
    "\n",
    "- Impala (Cloudera)\n",
    "- Instance\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J\n",
    "\n",
    "- Java Virtual Machine (JVM)\n",
    "- Jenkins\n",
    "- Jetty\n",
    "- jHispter\n",
    "- Jira\n",
    "    - vs. ServiceNow (\"SeriveNow\"?)\n",
    "    - vs. Slack Approval\n",
    "- jFrog\n",
    "- Julia\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K\n",
    "\n",
    "- Kafka (Apache)\n",
    "- Kendo\n",
    "- Key Performance Indicator (KPI)\n",
    "- Kibana\n",
    "- Kubernetes\n",
    "    - vs. Docker\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L\n",
    "\n",
    "- Lambda Program\n",
    "- Launch (Adobe)\n",
    "- \"Legacy\" Code\n",
    "- Lint\n",
    "- Load Test\n",
    "- Logstash\n",
    "- Low Level Programming vs. High Level Programming\n",
    "    - Psuedocode\n",
    "    - Spaghetti Code\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M\n",
    "\n",
    "- __MapReduce__ (_big data_)\n",
    "    - Hadoop's main tool/framework for processing data. MapReduce splits, filters, and organizes the data (the \"_map_\"), and performs an operation on the data (the \"reduce\"), all the while managing the location of the data among the distributed servers and the communications in between them.\n",
    "    - Spark is a faster tool because of in-memory real-time processing, whereas MapReduce uses batch processing, but it is more expensive.\n",
    "\n",
    "\n",
    "- Marathon\n",
    "- Maturity Model\n",
    "    - AIMM\n",
    "    - Capability Mat Model\n",
    "    - TPX\n",
    "- Mesos\n",
    "- mobilenetV2 (a CNN)\n",
    "- MongoDB\n",
    "    - vs. Redis/ElastiCache\n",
    "    - vs. Cassandra\n",
    "    - vs. HBase\n",
    "- Multi-thread Programming\n",
    "- MVS\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N\n",
    "\n",
    "- Native Development\n",
    "- .NET (Windows Framework)\n",
    "- Netezza\n",
    "- New Relic\n",
    "- Null Pointer Exception\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O\n",
    "\n",
    "- OBI\n",
    "- Object Oriented Programming\n",
    "- Objective-C\n",
    "- O-data\n",
    "- OLAP\n",
    "- Oozie (Apache)\n",
    "- Openfast Project\n",
    "    - Openfax\n",
    "    - Fargate\n",
    "- OpenShift\n",
    "- Optimizely\n",
    "- Oracle Unified Method\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P\n",
    "\n",
    "- Paloomey (_sp_)\n",
    "- Pearson Correlations (compare with pd.corr())\n",
    "- Perl\n",
    "- Phoenix (Apache)\n",
    "\n",
    "\n",
    "- __Pig__ (_big data_)\n",
    "    - Open source querying framework that is used ontop of HDFS to read and process data from HDFS for analysis. \n",
    "    - It uses its own \"Pig Latin\" programming language, which is similar to (and arguably easier to understand than) SQL.\n",
    "        - Good for quick processing of large amounts of incoming data, which is why Yahoo, Google, and Microsoft use Pig for their data scrapers.\n",
    "\n",
    "\n",
    "- Pivotal\n",
    "- Progressive Web Application (PWA)\n",
    "    - Cordova\n",
    "- Prometheus\n",
    "- Puredata\n",
    "- Push (upload)\n",
    "    - v. Pull (request)\n",
    "- Python\n",
    "    - vs. R\n",
    "    - vs. Scala\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q\n",
    "\n",
    "- __Qlik__ (_Business Intelligence_)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R\n",
    "\n",
    "- Recursive Feature Elimination\n",
    "\n",
    "\n",
    "- __Relational Database Management System (RDBMS)__ (_database_)\n",
    "    - Database where each item of data is structured and ordered \"in relation to\" other rows of data, thus creating schemas and tables. For example, if one row of data is deleted, everything else in relation to it is moved up or down accordingly.\n",
    "    - RDBMS's are constrasted from \"NoSQL\" management systems such as HBase. Differences found here (MR = \"MapReduce\"):\n",
    "    \n",
    "<img src='https://www.ironsidegroup.com/wp-content/uploads/2015/11/HBase-vs-RDBMS.jpg'>\n",
    "\n",
    "\n",
    "- (Code) Refactoring\n",
    "- Regime [(link)](https://math.stackexchange.com/questions/2562374/why-do-we-use-the-word-regime-in-math-science-engineering-instead-of-region)\n",
    "- Regression Cycle\n",
    "- Requirement\n",
    "    - (and Requirements Elicitation)\n",
    "    \n",
    "    \n",
    "- __Resilient Distributed Datasets (RDD)__ (_big data_)\n",
    "    - The main unit data structure of Spark - fancy term for a read-only collection of datasets. Spark partitions the RDD into individual datasets, each of which can be computed by a different node in a cluster.\n",
    "    \n",
    "    \n",
    "- REST\n",
    "- R-Studio\n",
    "- R2\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S\n",
    "\n",
    "- SAP\n",
    "- SAS\n",
    "    - vs. SPSS\n",
    "    - vs. SSIS\n",
    "    - vs. SSRS\n",
    "- Scala\n",
    "- Schema\n",
    "- sc-Lang\n",
    "- SDLC\n",
    "- Se\n",
    "- Selenium\n",
    "- Shell & Shell Scripting\n",
    "- Smoke Tests\n",
    "- Snowflake (co)\n",
    "- SOAP\n",
    "- SoIr (Apache)\n",
    "- SonicPi\n",
    "- Source Code\n",
    "\n",
    "\n",
    "- __Spark__ (Apache) (_big data_)\n",
    "    - Open source framework that uses Hadoop for storage, but unlike Hadoop, computes big data in real time (i.e., \"low latency computing\"). Downside is that is costlier due to in-memory solutions.\n",
    "    - Was created to be a faster (but more expensive) processing tool than Hadoop's MapReduce.\n",
    "    - __PySpark__ is a Python-API for Spark (A Scala-based tool) to use Python and Spark together. PySpark is its own programming language.\n",
    "    \n",
    "    \n",
    "    - vs. Hadoop\n",
    "    - vs. Hive\n",
    "    - vs. \"Spark 2\"\n",
    "- Spinnaker\n",
    "- Splunk\n",
    "- Spotfire\n",
    "- Sprint Planning/\"Sprinting\"\n",
    "- SQL\n",
    "  - NoSQL\n",
    "  - OracleSQL\n",
    "  - PL SQL\n",
    "  - RDBMS (PostgreSQL) vs. non RDBMS (MySQL)\n",
    "  - T SQL\n",
    "  \n",
    "  \n",
    "- __Sqoop__ (Apache) (_big data_)\n",
    "    - Program/framework that transfers bulk data between Hadoop and more traditional/structured data stores such as relational databases. Good for importing/exporting and extracting data.\n",
    "    \n",
    "    \n",
    "- Stream Processing\n",
    "- Statefulness\n",
    "- Storm (Apache)\n",
    "- SumoLogic\n",
    "- Supercollider\n",
    "- Survival Analysis\n",
    "- Swift\n",
    "  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T\n",
    "\n",
    "- Tableau (VISUALIZATION PROGRAMS)\n",
    "    - vs. Looker\n",
    "    - vs. Amazon Quicksight\n",
    "    - vs. Grafana\n",
    "- Target (Adobe)\n",
    "- Telegraf\n",
    "- Teradata\n",
    "- Terraform\n",
    "- Test-Driven Development (TDD)\n",
    "- Thrift (API)\n",
    "- TitanGraph\n",
    "- Tomcat Server\n",
    "- Toyo\n",
    "- Tracing\n",
    "- Tripwire\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U\n",
    "\n",
    "- Unified Modeling Language\n",
    "    - Stereotype (under UML)\n",
    "    - vs. Systems Modeling Language (SysML)\n",
    "- Unified Process\n",
    "- Unit Testing\n",
    "- Unity\n",
    "- Universal Disk Format (\".udf\")\n",
    "- Unix\n",
    "- Use Case\n",
    "    - vs. Abuse Case\n",
    "    - vs. Business Case\n",
    "    - vs. Misuse Case\n",
    "    - vs. Test Case\n",
    "    - Use Case Points\n",
    "    - User Story\n",
    "- UX and UX Design\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V\n",
    "\n",
    "- VBA \n",
    "    - Excel VBA\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W\n",
    "\n",
    "- Waterfall (Method)\n",
    "    - vs. Agile/Scrum\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X\n",
    "\n",
    "- X2 Test\n",
    "- Xamarin\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y\n",
    "\n",
    "- YARN\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z\n",
    "\n",
    "- Zanzibar\n",
    "- ZooKeeper (Apache)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Notes for Potential Organization:\n",
    "\n",
    "- __Build (Actual Development)__\n",
    "    - Jenkins\n",
    "    - Bamboo\n",
    "    - jFrog\n",
    "<br>\n",
    "\n",
    "\n",
    "- __Testing (Quality Assurance)__\n",
    "    - Jenkins\n",
    "    - Bamboo\n",
    "    - Se\n",
    "<br>\n",
    "\n",
    "\n",
    "- __Deployment (requires mixture of IT and DevOps skills)__\n",
    "    - Use custom scripts to make this happen\n",
    "    - Containering methods (Docker/Kubernetes) may have ability to regulate deployment as well\n",
    "    - _ALWAYS_ good to deploy on the weekends, in case you need to rollback deployment, or if it takes hours (to even days) to deploy.\n",
    "<br>\n",
    "\n",
    "\n",
    "- __Verify Post-Deployment (& Potential Rollback)__\n",
    "    - Appdynamics\n",
    "    - Splunk\n",
    "    - New Relic\n",
    "    - Elastic\n",
    "    - SumoLogic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
